{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9814cc89",
   "metadata": {},
   "source": [
    "# FastText and Word Vector (TP n°3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a85323ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "from nltk import tokenize\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "import spacy\n",
    "import fasttext as fast\n",
    "#import transformers\n",
    "\n",
    "from typing import Dict\n",
    "from typing import Callable\n",
    "from typing import List\n",
    "import re\n",
    "\n",
    "# set a defined random generator, better for reproducible results.\n",
    "random = np.random.default_rng(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49829ed2",
   "metadata": {},
   "source": [
    "## Take a look on IMDB dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c3efb039",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset imdb (/home/leherlemaxime/.cache/huggingface/datasets/imdb/plain_text/1.0.0/e3c66f1788a67a89c7058d97ff62b6c30531e05b549de56d3ab91891f0561f9a)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6f2711bb3ea4140a3f6724ba672b511",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 25000\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 25000\n",
      "    })\n",
      "    unsupervised: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 50000\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "imdb = load_dataset('imdb')\n",
    "print(imdb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87525b57",
   "metadata": {},
   "source": [
    "And we have the following number of entries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5d2e6ba5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train entries: 25000\n",
      "test entries: 25000\n"
     ]
    }
   ],
   "source": [
    "print(f\"train entries: {len(imdb['train'])}\\ntest entries: {len(imdb['test'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81f04a2f",
   "metadata": {},
   "source": [
    "## Translate dataset for FastText API:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1254f12f",
   "metadata": {},
   "source": [
    "Generate a shuffle index list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "198a8c87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10757  3394  5269 14177  2703 16747 24521  7483   580 19241]\n"
     ]
    }
   ],
   "source": [
    "rand_idx = np.arange(len(imdb['train']))\n",
    "np.random.shuffle(rand_idx)\n",
    "print(rand_idx[:10])\n",
    "rand_idy = np.arange(len(imdb['train']))\n",
    "np.random.shuffle(rand_idy)\n",
    "print(rand_idy[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1f92ab7",
   "metadata": {},
   "source": [
    "Write IMDB dataset into file with FastText format:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bdaf3212",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.03 ms, sys: 550 µs, total: 1.58 ms\n",
      "Wall time: 1.47 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "if not os.path.exists(\"imdb_train.txt\"):\n",
    "    with open(\"imdb_train.txt\", \"wb\") as f:\n",
    "        for i in rand_idx:\n",
    "            entry = imdb['train'][int(i)]\n",
    "            s = f\"__label__{entry['label']} {entry['text']}\\n\".encode(\"utf-8\")\n",
    "            f.write(s)\n",
    "    \n",
    "        f.close()\n",
    "        \n",
    "if not os.path.exists(\"imdb_test.txt\"):\n",
    "    with open(\"imdb_test.txt\", \"wb\") as f:\n",
    "        for i in rand_idy:\n",
    "            entry = imdb['test'][int(i)]\n",
    "            s = f\"__label__{entry['label']} {entry['text']}\\n\".encode(\"utf-8\")\n",
    "            f.write(s)\n",
    "    \n",
    "        f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84d4c36c",
   "metadata": {},
   "source": [
    "Let's see the input format of an entry:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d322b72b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__label__0 Omen IV: The Awakening starts at the 'St. Frances Orphanage' where husband & wife Karen (Faye Grant) & Gene York (Michael Woods) are given a baby girl by Sister Yvonne (Megan Leitch) who they have adopted, they name her Delia. At first things go well but as the years pass & Delia (Asia Vieria) grows up Karen becomes suspicious of her as death & disaster follows her, Karen is convinced that she is evil itself. Karen then finds out that she is pregnant but discovers a sinister plot to use her as a surrogate mother for th next Antichrist & gets a shock when she finds out who Delia's real father was...<br /><br />Originally to be directed by Dominique Othenin-Girard who either quit or was sacked & was replaced by Jorge Montesi who completed the film although why he bothered is anyone's guess as Omen IV: The Awakening is absolutely terrible & a disgrace when compared to it illustrious predecessors. The script by Brian Taggert is hilariously bad, I'm not sure whether this nonsense actually looked good as the written word on a piece of paper but there are so many things wrong with it that I find even that hard to believe. As a serious film Omen IV: The AWakening falls flat on it's face & it really does work better if you look at it as a comedy spoof, I mean the scene towards the end when the Detective comes face-to-face with a bunch of zombie carol singers who are singing an ominous Gothic song has to be seen to be believed & I thought it was absolutely hilarious & ridiculous in equal measure. Then there's the pointless difference between this & the other Omen films in that this time it's a young girl, the question I ask here is why? Seriously, why? There's no reason at all & isn't used to any effect at all anyway. Then of course there's the stupid twist at the end which claims Delia has been keeping her brother's embryo inside herself & that in a sinister conspiracy involving a group of Satan worshippers it has been implanted in Karen so she can give birth to the Antichrist is moronic & comes across as just plain daft. At first it has a certain entertainment value in how bad it is but the unintentional hilarity gives way to complete boredom sooner rather than later.<br /><br />It's obviously impossible to know how much of Omen IV: The Awakening was directed by Girard & Montesi but you can sort of tell all was not well behind the camera as it's a shabby, cheap looking poorly made film which was actually made-for-TV & it shows with the bland, flat & unimaginative cinematography & production design. Then there's the total lack of scares, atmosphere, tension & gore which are the main elements that made the previous Omen films so effective.<br /><br />The budget must have been pretty low & the film looks like it was. The best most stylish thing about Omen IV: The Awakening is the final shot in which the camera rises up in the air as Delia walks away into the distance to reveal a crucifix shaped cross made by two overlapping path's but this is the very last shot before the end credits roll which says just about everything. I have to mention the music which sounds awful, more suited to a comedy & is very inappropriate sounding. The acting is alright at best but as usual the kid annoys.<br /><br />Omen IV: The Awakening is rubbish, it's a totally ridiculous film that tries to be serious & just ends up coming across as stupid. The change of director's probably didn't help either, that's still not a excuse though. The last Omen film to date following the original The Omen (1976), Damien: Omen II (1978) & The Final Conflict (1981) all of which are far superior to this.\r\n"
     ]
    }
   ],
   "source": [
    "!head -n 1 imdb_train.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17f8d696",
   "metadata": {},
   "source": [
    "## First training with FastText model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4ac30e68",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Read 5M words\n",
      "Number of words:  281132\n",
      "Number of labels: 2\n",
      "Progress: 100.0% words/sec/thread: 1399773 lr:  0.000000 avg.loss:  0.425937 ETA:   0h 0m 0s\n"
     ]
    }
   ],
   "source": [
    "fast_model = fast.train_supervised('imdb_train.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "131052e8",
   "metadata": {},
   "source": [
    "Let's see the train vocabulary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5ed80c14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the vocabulary size is: 281132\n",
      "\n",
      "This is a slice of it:\n",
      "['the', 'a', 'and', 'of', 'to', 'is', 'in', 'I', 'that', 'this', 'it', '/><br', 'was', 'as', 'with', 'for', 'but', 'The', 'on', 'movie']\n"
     ]
    }
   ],
   "source": [
    "print(f\"the vocabulary size is: {len(fast_model.words)}\\n\\nThis is a slice of it:\\n{fast_model.words[:20]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a6fe6b2",
   "metadata": {},
   "source": [
    "### Results of the model:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50f10eb9",
   "metadata": {},
   "source": [
    "We respectfully copy and paste this print function from FastText documentation to see results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b1373799",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_results(N : int, p : float, r : float) -> None:\n",
    "    print(\"N\\t\" + str(N))\n",
    "    print(\"P@{}\\t{:.3f}\".format(1, p))\n",
    "    print(\"R@{}\\t{:.3f}\".format(1, r))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99f8ded3",
   "metadata": {},
   "source": [
    "So let's compute precision at 1 (P@1) and the recall on the test dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e202d23e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N\t25000\n",
      "P@1\t0.859\n",
      "R@1\t0.859\n"
     ]
    }
   ],
   "source": [
    "print_results(*fast_model.test('imdb_test.txt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "892b6996",
   "metadata": {},
   "source": [
    "And we can compute these metrics for all labels separately:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d975f76a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_labels_results(l_scores : Dict[str, Dict[str, float]]) -> None:\n",
    "    for label in l_scores:\n",
    "        print(f\"label '{label}':\\n\")\n",
    "        print(f\"\\tprecision: {np.round(l_scores[label]['precision'], 3)}\")\n",
    "        print(f\"\\trecall: {np.round(l_scores[label]['recall'], 3)}\")\n",
    "        print(f\"\\tF1 score: {np.round(l_scores[label]['f1score'], 3)}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d6d7b27d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label '__label__1':\n",
      "\n",
      "\tprecision: 0.867\n",
      "\trecall: nan\n",
      "\tF1 score: 1.733\n",
      "\n",
      "label '__label__0':\n",
      "\n",
      "\tprecision: 0.852\n",
      "\trecall: nan\n",
      "\tF1 score: 1.704\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_labels_results(fast_model.test_label('imdb_test.txt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ae963c3",
   "metadata": {},
   "source": [
    "## Pre-processing on IMDB dataset:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c41984f3",
   "metadata": {},
   "source": [
    "### Clean the text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e28a7db7",
   "metadata": {},
   "source": [
    "The text-format is not perfect, we have for exemple '\\t' or '<br\\>' that are formated text. So we will replace all special char by space. And we will also add space before and after '!' to make it a separated word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cfc1d9e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_the_text(text_array : str) -> str:\n",
    "    '''\n",
    "        This function return a list of all word and char in the text in parameters.\n",
    "\n",
    "            Parameters:\n",
    "                    text_array (str): The text in a string format.\n",
    "\n",
    "            Returns:\n",
    "                    result (str) : A list with all the word and char in the inpt text.\n",
    "    '''\n",
    "    \n",
    "    specialChars = \"()\\\\\\''.,;:\\\"?-\" \n",
    "    for specialChar in specialChars:\n",
    "        text_array = text_array.replace(specialChar, ' ')\n",
    "        \n",
    "    text_array = text_array.replace(\"/>\", ' ')\n",
    "    text_array = text_array.replace(\"<br\", ' ')\n",
    "    \n",
    "    ''' We add space before and after '!' for the split function '''\n",
    "    text_array = text_array.replace(\"!\", \" ! \")\n",
    "    \n",
    "    return text_array.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b323835",
   "metadata": {},
   "source": [
    "Now we can try the same model but with the clean text and see if this modification change the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "98c92795",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.37 ms, sys: 0 ns, total: 1.37 ms\n",
      "Wall time: 1.26 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "if not os.path.exists(\"imdb_clean_train.txt\"):\n",
    "    with open(\"imdb_clean_train.txt\", \"wb\") as f:\n",
    "        for i in rand_idx:\n",
    "            entry = imdb['train'][int(i)]\n",
    "            s = f\"__label__{entry['label']} {clean_the_text(entry['text'])}\\n\".encode(\"utf-8\")\n",
    "            f.write(s)\n",
    "    \n",
    "        f.close()\n",
    "        \n",
    "if not os.path.exists(\"imdb_clean_test.txt\"):\n",
    "    with open(\"imdb_clean_test.txt\", \"wb\") as f:\n",
    "        for i in rand_idy:\n",
    "            entry = imdb['test'][int(i)]\n",
    "            s = f\"__label__{entry['label']} {clean_the_text(entry['text'])}\\n\".encode(\"utf-8\")\n",
    "            f.write(s)\n",
    "    \n",
    "        f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "210d0157",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__label__0 what a time we live in when someone like this joe swan whatever the hell is considered a good filmmaker   or even a filmmaker at all !  where are the new crop of filmmakers with brains and talent    we need them bad  and to hell with mumblecore !       this movie is about nothing  just as the characters in the film stand for nothing  it s this horrible  so called gen y  that is full of bored idiots  some of which declare themselves filmmakers with out bothering to learn anything about the craft before shooting  well  orson welles was a filmmaker  john huston was a filmmaker  fellini was a filmmaker  dreyer was a filmmaker  etc  current films like these show just how stupid young  so called  filmmakers  can be when they believe going out with no script  no direction  no thought  no legit  camerawork   everything shot horribly on dv   no craft of editing  no nothing  stands for  rebellious  or  advanced  film making  nope  it s called ignorance and laziness or just pure masturbation of cinema  and there actually is an in your face  jack off shot   so be ready         look at the early films of any accomplished  indie  filmmaker  linklatter  morris  allen  lynch  hartley  jarmusch  jost  lee  or herzog   none made anything as tedious and aimless as this  yet swan whatever the hell  is still going to sxsw every year and hailed as some kind of gutsy  new talent  it s crap !  i can t imagine anyone liking this  and everything else this so called filmmaker has done  all seen by me  is just as bad  the newer stuff clearly made to appeal to a more mainstream audience  one of the sitcom calling   steer clear  unless you re a friend or family member of those involved   on second thought  if you re a family member or friend you d probably be embarrassed to see a family member or friend in such compromising situations         utter garbage  this isn t art  this is the ultimate opposite of it \r\n"
     ]
    }
   ],
   "source": [
    "!head -n 1 imdb_clean_train.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6d5ff022",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Read 6M words\n",
      "Number of words:  80799\n",
      "Number of labels: 2\n",
      "Progress: 100.0% words/sec/thread: 1528910 lr:  0.000000 avg.loss:  0.389570 ETA:   0h 0m 0s100.0% words/sec/thread: 1529060 lr: -0.000007 avg.loss:  0.389570 ETA:   0h 0m 0s\n"
     ]
    }
   ],
   "source": [
    "fast_model_clean = fast.train_supervised('imdb_clean_train.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f13e1733",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the vocabulary size is: 80799\n",
      "\n",
      "This is a slice of it:\n",
      "['the', 'and', 'a', 'of', 'to', 'is', 'it', 'in', 'i', 'this', 'that', 's', 'was', 'as', 'for', 'with', 'movie', 'but', 'film', 'you']\n"
     ]
    }
   ],
   "source": [
    "print(f\"the vocabulary size is: {len(fast_model_clean.words)}\\n\\nThis is a slice of it:\\n{fast_model_clean.words[:20]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9647f111",
   "metadata": {},
   "source": [
    "#### Result of clean model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fe13767c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N\t25000\n",
      "P@1\t0.879\n",
      "R@1\t0.879\n"
     ]
    }
   ],
   "source": [
    "print_results(*fast_model_clean.test('imdb_clean_test.txt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e60ed600",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label '__label__0':\n",
      "\n",
      "\tprecision: 0.882\n",
      "\trecall: nan\n",
      "\tF1 score: 1.763\n",
      "\n",
      "label '__label__1':\n",
      "\n",
      "\tprecision: 0.876\n",
      "\trecall: nan\n",
      "\tF1 score: 1.752\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_labels_results(fast_model_clean.test_label('imdb_clean_test.txt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3549de4",
   "metadata": {},
   "source": [
    "So we can see that in average we have a upgrade of our result of 0.02. It's not a huge upgrade but it's still ok for a few more seconds of calculation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3e7b9c9",
   "metadata": {},
   "source": [
    "### Clean text with stop word"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35b3154a",
   "metadata": {},
   "source": [
    "We have see that the function to clean upgrade our result but why stop here ?\n",
    "\n",
    "We can add a other clean step on the text, this step is to delete stop words. What is stop words ? Stop words are the non-discriminating words, like __the__, __a__, __an__, __this__ ....\n",
    "\n",
    "So first of all we will create a list of all the stop word and after we will delete them from text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "99de6205",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_stop_word = [\"the\", \"and\", \"a\", \"of\", \"to\", \"is\", \"it\", \"in\", \"this\", \"that\", \"s\", \"was\", \"as\", \"for\", \"with\", \"but\", \"then\", \"an\", \"at\", \"who\", \"when\", \"than\", \"where\", \"which\", \"with\", \"on\", \"t\", \"are\", \"by\", \"so\", \"from\", \"have\", \"be\", \"or\", \"just\", \"about\", \"\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a81e3a4",
   "metadata": {},
   "source": [
    "Now we will create our extend clean text function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3354de88",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_the_text_extend(text : str, list_of_stop_word : List[str]) -> str:\n",
    "    '''\n",
    "        This function return a list of all word and char in the text in parameters.\n",
    "\n",
    "            Parameters:\n",
    "                    text (str): The text in a string format.\n",
    "                    \n",
    "                    list_of_stop_word: This is that list of our stop words to remove from the text\n",
    "\n",
    "            Returns:\n",
    "                    result (str) : A list with all the word and char in the inpt text.\n",
    "    '''\n",
    "    text = text.lower()\n",
    "    \n",
    "    specialChars = \"()\\\\\\''.,;:\\\"?-\" \n",
    "    for specialChar in specialChars:\n",
    "        text = text.replace(specialChar, ' ')\n",
    "        \n",
    "    text = text.replace(\"/>\", ' ')\n",
    "    text = text.replace(\"<br\", ' ')\n",
    "    \n",
    "    text = text.replace(\"</s>\", \" \")\n",
    "    \n",
    "    ''' We add space before and after '!' for the split function '''\n",
    "    text = text.replace(\"!\", \" ! \")\n",
    "    \n",
    "    for word in list_of_stop_word:\n",
    "        ''' We add this to only remove the all word and not isolated letter in an other word'''\n",
    "        word = \" \" + word + \" \"\n",
    "        text = text.replace(word, \" \")\n",
    "    \n",
    "    return text.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e3d8d45",
   "metadata": {},
   "source": [
    "Now try again with this new function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b7446453",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.39 ms, sys: 145 µs, total: 1.54 ms\n",
      "Wall time: 1.44 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "if not os.path.exists(\"imdb_clean_extend_train.txt\"):\n",
    "    with open(\"imdb_clean_extend_train.txt\", \"wb\") as f:\n",
    "        for i in rand_idx:\n",
    "            entry = imdb['train'][int(i)]\n",
    "            s = f\"__label__{entry['label']} {clean_the_text_extend(entry['text'], list_of_stop_word)}\\n\".encode(\"utf-8\")\n",
    "            f.write(s)\n",
    "    \n",
    "        f.close()\n",
    "\n",
    "if not os.path.exists(\"imdb_clean_extend_test.txt\"):\n",
    "    with open(\"imdb_clean_extend_test.txt\", \"wb\") as f:\n",
    "        for i in rand_idy:\n",
    "            entry = imdb['test'][int(i)]\n",
    "            s = f\"__label__{entry['label']} {clean_the_text_extend(entry['text'], list_of_stop_word)}\\n\".encode(\"utf-8\")\n",
    "            f.write(s)\n",
    "    \n",
    "        f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "802cc364",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__label__0 this yawn titles credits  boring point tedium acting wooden stilted !  admittedly director richard jobson directing debut  earth green lit script poorly developed one  looks like another money down drain government project  scottish screen credited surprise  surprise   nearly fell asleep three times my review will unfortunately more restrained one  please  please mister jobson what ever you ve been doing prior directing sedative  go back ! \r\n"
     ]
    }
   ],
   "source": [
    "!head -n 1 imdb_clean_extend_train.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ad6ae041",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Read 3M words\n",
      "Number of words:  80799\n",
      "Number of labels: 2\n",
      "Progress: 100.0% words/sec/thread: 1472403 lr:  0.000000 avg.loss:  0.325895 ETA:   0h 0m 0s\n"
     ]
    }
   ],
   "source": [
    "fast_model_clean_extend = fast.train_supervised('imdb_clean_extend_train.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4c9a2f03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the vocabulary size is: 80799\n",
      "\n",
      "This is a slice of it:\n",
      "['you', 'not', 'one', '</s>', '!', 'all', 'they', 'like', 'there', 'or', 'just', 'about', 'out', 'if', 'has', 'what', 'some', 'good', 'can', 'more']\n"
     ]
    }
   ],
   "source": [
    "print(f\"the vocabulary size is: {len(fast_model_clean_extend.words)}\\n\\nThis is a slice of it:\\n{fast_model_clean_extend.words[:20]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21e53bc3",
   "metadata": {},
   "source": [
    "#### Result of clean model extend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "da8ee6ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N\t25000\n",
      "P@1\t0.886\n",
      "R@1\t0.886\n"
     ]
    }
   ],
   "source": [
    "print_results(*fast_model_clean_extend.test('imdb_clean_extend_test.txt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "bbbee235",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label '__label__1':\n",
      "\n",
      "\tprecision: 0.891\n",
      "\trecall: nan\n",
      "\tF1 score: 1.781\n",
      "\n",
      "label '__label__0':\n",
      "\n",
      "\tprecision: 0.881\n",
      "\trecall: nan\n",
      "\tF1 score: 1.762\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_labels_results(fast_model_clean_extend.test_label('imdb_clean_extend_test.txt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82194d0c",
   "metadata": {},
   "source": [
    "So with this result we can see that the result are abit better so we will now use the clean text expand instead of clean text classic."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60794076",
   "metadata": {},
   "source": [
    "### Stemming the data:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7b643a8",
   "metadata": {},
   "source": [
    "First of all we need to create a function that stemme a word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "77e4fac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "re_word = re.compile(r\"^\\w+$\")\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "\n",
    "stem_word: Callable[[str], str] = lambda w : stemmer.stem(w.lower()) if re_word.match(w) else w"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "545dd43c",
   "metadata": {},
   "source": [
    "Now we have ti create a function that apply stemming to a whole text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4f981507",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stemming_text(text : str) -> str:\n",
    "    '''\n",
    "        This function steeming the text in parameter and return in\n",
    "        \n",
    "        Parameters :\n",
    "                text (str) : the text to stemming\n",
    "                \n",
    "        Returns :\n",
    "                return_text (str) : the text stemmed\n",
    "    '''\n",
    "    list_of_words = text.split(\" \")\n",
    "    \n",
    "    list_of_words = [stem_word(word) for word in list_of_words]\n",
    "    \n",
    "    return_text = \" \".join(list_of_words)\n",
    "    \n",
    "    return return_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08f2d1c6",
   "metadata": {},
   "source": [
    "Now with this function we can create the new model where we use the stemming for all text before write them in the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a3da7b59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.24 ms, sys: 110 µs, total: 1.35 ms\n",
      "Wall time: 1.24 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "if not os.path.exists(\"imdb_stemmed_train.txt\"):\n",
    "    with open(\"imdb_stemmed_train.txt\", \"wb\") as f:\n",
    "        for i in rand_idx:\n",
    "            entry = imdb['train'][int(i)]\n",
    "            s = f\"__label__{entry['label']} {stemming_text(entry['text'])}\\n\".encode(\"utf-8\")\n",
    "            f.write(s)\n",
    "    \n",
    "        f.close()\n",
    "        \n",
    "if not os.path.exists(\"imdb_stemmed_test.txt\"):\n",
    "    with open(\"imdb_stemmed_test.txt\", \"wb\") as f:\n",
    "        for i in rand_idy:\n",
    "            entry = imdb['test'][int(i)]\n",
    "            s = f\"__label__{entry['label']} {stemming_text(entry['text'])}\\n\".encode(\"utf-8\")\n",
    "            f.write(s)\n",
    "    \n",
    "        f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b2a41939",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__label__1 i watch this last night after not have seen it for sever years. it realli is a fun littl film, with a bunch of face you didn't know were in it. arkin shine as always. check it out; you won't be dissappointed. by the way, it was just releas on dvd and contrari to it packaging, it is widescreen. the transfer is rather poor, but at least the whole movi is visible. ;-)\r\n"
     ]
    }
   ],
   "source": [
    "!head -n 1 imdb_stemmed_train.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e596fd3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Read 5M words\n",
      "Number of words:  245430\n",
      "Number of labels: 2\n",
      "Progress: 100.0% words/sec/thread: 1434439 lr:  0.000000 avg.loss:  0.423211 ETA:   0h 0m 0s\n"
     ]
    }
   ],
   "source": [
    "fast_model_stemmed = fast.train_supervised('imdb_stemmed_train.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e79a8d61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the vocabulary size is: 245430\n",
      "\n",
      "This is a slice of it:\n",
      "['the', 'a', 'and', 'of', 'to', 'is', 'in', 'it', 'i', 'this', 'that', '/><br', 'was', 'as', 'for', 'with', 'but', 'movi', 'film', 'be']\n"
     ]
    }
   ],
   "source": [
    "print(f\"the vocabulary size is: {len(fast_model_stemmed.words)}\\n\\nThis is a slice of it:\\n{fast_model_stemmed.words[:20]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48116788",
   "metadata": {},
   "source": [
    "#### Result of stemmed model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "bbdcb864",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N\t25000\n",
      "P@1\t0.861\n",
      "R@1\t0.861\n"
     ]
    }
   ],
   "source": [
    "print_results(*fast_model_stemmed.test('imdb_stemmed_test.txt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "8a8cd6e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label '__label__1':\n",
      "\n",
      "\tprecision: 0.86\n",
      "\trecall: nan\n",
      "\tF1 score: 1.72\n",
      "\n",
      "label '__label__0':\n",
      "\n",
      "\tprecision: 0.863\n",
      "\trecall: nan\n",
      "\tF1 score: 1.726\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_labels_results(fast_model_stemmed.test_label('imdb_stemmed_test.txt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d283b8c0",
   "metadata": {},
   "source": [
    "The stemmed model don't change the result or juste of 0.002 in the label 1 so the result is not convincing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27ee55bb",
   "metadata": {},
   "source": [
    "### Lemming the data:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67dfb962",
   "metadata": {},
   "source": [
    "Firstly, we need to download the english model of Spacy lemmatization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "50012030",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m spacy download en_core_web_sm > output_dl.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "5bc8baf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading the small English model\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "152368f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.26 ms, sys: 170 µs, total: 2.43 ms\n",
      "Wall time: 1.87 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "if not os.path.exists(\"lemmed_imdb_train.txt\"):\n",
    "    with open(\"lemmed_imdb_train.txt\", \"wb\") as f:\n",
    "        for i in rand_idx:\n",
    "            entry = imdb['train'][int(i)]\n",
    "            \n",
    "            # lemmatize before writting\n",
    "            lemmed_text = ' '.join([token.lemma_ for token in nlp(entry['text'])])\n",
    "            s = f\"__label__{entry['label']} {lemmed_text}\\n\".encode(\"utf-8\")\n",
    "            f.write(s)\n",
    "    \n",
    "        f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb82b8a6",
   "metadata": {},
   "source": [
    "Do it in test dataset also:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "22937706",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(\"lemmed_imdb_test.txt\"):\n",
    "    with open(\"lemmed_imdb_test.txt\", \"wb\") as f:\n",
    "        for i in rand_idy:\n",
    "            entry = imdb['test'][int(i)]\n",
    "            \n",
    "            # lemmatize before writting\n",
    "            lemmed_text = ' '.join([token.lemma_ for token in nlp(entry['text'])])\n",
    "            s = f\"__label__{entry['label']} {lemmed_text}\\n\".encode(\"utf-8\")\n",
    "            f.write(s)\n",
    "    \n",
    "        f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "8acdb4b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Read 6M words\n",
      "Number of words:  106199\n",
      "Number of labels: 2\n",
      "Progress: 100.0% words/sec/thread: 1797330 lr:  0.000000 avg.loss:  0.414804 ETA:   0h 0m 0s\n"
     ]
    }
   ],
   "source": [
    "fast_model_lemming = fast.train_supervised('lemmed_imdb_train.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "6d87da66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the vocabulary size is: 106199\n",
      "\n",
      "This is a slice of it:\n",
      "['the', 'be', ',', '.', 'and', 'a', 'of', 'to', 'it', 'I', 'in', 'this', 'that', '\"', 'have', '-', '/><br', 'movie', 'film', 'as']\n"
     ]
    }
   ],
   "source": [
    "print(f\"the vocabulary size is: {len(fast_model_lemming.words)}\\n\\nThis is a slice of it:\\n{fast_model_lemming.words[:20]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8793d9e0",
   "metadata": {},
   "source": [
    "#### Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e5875f31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N\t25000\n",
      "P@1\t0.867\n",
      "R@1\t0.867\n"
     ]
    }
   ],
   "source": [
    "print_results(*fast_model_lemming.test('lemmed_imdb_test.txt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "5881726e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label '__label__0':\n",
      "\n",
      "\tprecision: 0.868\n",
      "\trecall: nan\n",
      "\tF1 score: 1.736\n",
      "\n",
      "label '__label__1':\n",
      "\n",
      "\tprecision: 0.867\n",
      "\trecall: nan\n",
      "\tF1 score: 1.734\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_labels_results(fast_model_lemming.test_label('lemmed_imdb_test.txt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4368f4ed",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7404829f",
   "metadata": {},
   "source": [
    "## Hyperparameters tunning:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf2a357e",
   "metadata": {},
   "source": [
    "We need to extract a validation set of our train dataset to avoid a tunning validation on test dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "02366b7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-r--r-- 1 leherlemaxime leherlemaxime 26719815 Oct  5 16:38 ./tuning_aa\r\n",
      "-rw-r--r-- 1 leherlemaxime leherlemaxime  6713008 Oct  5 16:38 ./tuning_ab\r\n",
      "-rw-r--r-- 1 leherlemaxime leherlemaxime 19853802 Oct  5 16:14 ./tuning_complet_aa\r\n",
      "-rw-r--r-- 1 leherlemaxime leherlemaxime  4974531 Oct  5 16:14 ./tuning_complet_ab\r\n"
     ]
    }
   ],
   "source": [
    "# split command will copy and separate file into set of files of 20000 lines.\n",
    "# Train file have 25.000 lines, so train will have 20.000 lines and validation 5.000 lines.\n",
    "!split -l20000 \"imdb_train.txt\" tuning_\n",
    "\n",
    "!ls -l ./tuning_*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e26d837",
   "metadata": {},
   "source": [
    "### Try the default hyperparameter tunning of FastText:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "3f315ad8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Progress: 100.0% Trials:    9 Best score:  0.885598 ETA:   0h 0m 0s\n",
      "Training again with best arguments\n",
      "Read 4M words\n",
      "Number of words:  244423\n",
      "Number of labels: 2\n",
      "Progress: 100.0% words/sec/thread:  638637 lr:  0.000000 avg.loss:  0.047441 ETA:   0h 0m 0s\n"
     ]
    }
   ],
   "source": [
    "tunning_fast_model = fast.train_supervised(input='tuning_aa', autotuneValidationFile='tuning_ab', autotuneMetric=\"f1:__label__0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72fcbfef",
   "metadata": {},
   "source": [
    "Let's compute global metrics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "b0845727",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N\t25000\n",
      "P@1\t0.883\n",
      "R@1\t0.883\n"
     ]
    }
   ],
   "source": [
    "print_results(*tunning_fast_model.test('imdb_test.txt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69d8a41a",
   "metadata": {},
   "source": [
    "It looks to give better results with default hyperparameter tunning. But how labels scores change ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "65768679",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label '__label__1':\n",
      "\n",
      "\tprecision: 0.882\n",
      "\trecall: nan\n",
      "\tF1 score: 1.764\n",
      "\n",
      "label '__label__0':\n",
      "\n",
      "\tprecision: 0.885\n",
      "\trecall: nan\n",
      "\tF1 score: 1.769\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_labels_results(tunning_fast_model.test_label('imdb_test.txt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8508810",
   "metadata": {},
   "source": [
    "Results are better with tunning and we highlight that optimize f1 result on __negative__ label induces better improvements on __positive__ label. The reason is because we juste have two labels and __negative__ label had less wrongly classification."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af29af9e",
   "metadata": {},
   "source": [
    "## Merge optimisation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a6f8061",
   "metadata": {},
   "source": [
    "First we will try to add the clean text extend optimisation to other optimisaion because we see that this optimisation clean the text and just keep this important part. We also see that lemming is much better that stemming.\n",
    "\n",
    "It's the reason why we try a model with clean text extend and lemming."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "71fc2195",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 158 µs, sys: 10 ms, total: 10.2 ms\n",
      "Wall time: 7.6 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "if not os.path.exists(\"lemmed_clean_imdb_train.txt\"):\n",
    "    with open(\"lemmed_clean_imdb_train.txt\", \"wb\") as f:\n",
    "        for i in rand_idx:\n",
    "            entry = imdb['train'][int(i)]\n",
    "            \n",
    "            # lemmatize before writting\n",
    "            lemmed_text = ' '.join([token.lemma_ for token in nlp(clean_the_text_extend(entry['text'], list_of_stop_word))])\n",
    "            s = f\"__label__{entry['label']} {lemmed_text}\\n\".encode(\"utf-8\")\n",
    "            f.write(s)\n",
    "    \n",
    "        f.close()\n",
    "\n",
    "if not os.path.exists(\"lemmed_clean_imdb_test.txt\"):\n",
    "    with open(\"lemmed_clean_imdb_test.txt\", \"wb\") as f:\n",
    "        for i in rand_idy:\n",
    "            entry = imdb['test'][int(i)]\n",
    "            \n",
    "            # lemmatize before writting\n",
    "            lemmed_text = ' '.join([token.lemma_ for token in nlp(clean_the_text_extend(entry['text'], list_of_stop_word))])\n",
    "            s = f\"__label__{entry['label']} {lemmed_text}\\n\".encode(\"utf-8\")\n",
    "            f.write(s)\n",
    "    \n",
    "        f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "2f641576",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Read 3M words\n",
      "Number of words:  64059\n",
      "Number of labels: 2\n",
      "Progress: 100.0% words/sec/thread: 1685913 lr:  0.000000 avg.loss:  0.327277 ETA:   0h 0m 0s\n"
     ]
    }
   ],
   "source": [
    "fast_model_lemming_clean = fast.train_supervised('lemmed_clean_imdb_train.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "f46477f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the vocabulary size is: 64059\n",
      "\n",
      "This is a slice of it:\n",
      "['you', 'not', 'they', 'have', 'be', 'one', 'do', '</s>', '!', 'all', 'see', 'make', 'like', 'good', 'there', 'well', 'or', 'just', 'about', 'out']\n"
     ]
    }
   ],
   "source": [
    "print(f\"the vocabulary size is: {len(fast_model_lemming_clean.words)}\\n\\nThis is a slice of it:\\n{fast_model_lemming_clean.words[:20]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42602797",
   "metadata": {},
   "source": [
    "#### Results of combine optimisations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "7e54b70c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N\t25000\n",
      "P@1\t0.880\n",
      "R@1\t0.880\n"
     ]
    }
   ],
   "source": [
    "print_results(*fast_model_lemming_clean.test('lemmed_clean_imdb_test.txt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "5701fde8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label '__label__0':\n",
      "\n",
      "\tprecision: 0.875\n",
      "\trecall: nan\n",
      "\tF1 score: 1.75\n",
      "\n",
      "label '__label__1':\n",
      "\n",
      "\tprecision: 0.884\n",
      "\trecall: nan\n",
      "\tF1 score: 1.768\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_labels_results(fast_model_lemming_clean.test_label('lemmed_clean_imdb_test.txt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46872a8c",
   "metadata": {},
   "source": [
    "So with these 2 optimisation we have not a improvement compare to clean text extended only. OUr hypothesys is that we start to overfit on the train set. So a solution is to use this methode but with tunning hyperparametres."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee5fd9be",
   "metadata": {},
   "source": [
    "## Our final model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "f94cbb63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-r--r-- 1 leherlemaxime leherlemaxime 19853802 Oct  5 16:45 ./tuning_complet_aa\r\n",
      "-rw-r--r-- 1 leherlemaxime leherlemaxime  4974531 Oct  5 16:45 ./tuning_complet_ab\r\n"
     ]
    }
   ],
   "source": [
    "# split command will copy and separate file into set of files of 20000 lines.\n",
    "# Train file have 25.000 lines, so train will have 20.000 lines and validation 5.000 lines.\n",
    "!split -l20000 \"lemmed_clean_imdb_train.txt\" tuning_complet_\n",
    "\n",
    "!ls -l ./tuning_complet_*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a376a6f7",
   "metadata": {},
   "source": [
    "We have manually work on the parameter epochs, rate and n-grams. But we don't succed in find a better resulat that juste use autometric so we juste use this to get our better result on this data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "8a0ff0e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Progress: 100.0% Trials:    9 Best score:  0.893574 ETA:   0h 0m 0s\n",
      "Training again with best arguments\n",
      "Read 3M words\n",
      "Number of words:  58103\n",
      "Number of labels: 2\n",
      "Progress: 100.0% words/sec/thread:  414091 lr:  0.000000 avg.loss:  0.034724 ETA:   0h 0m 0s\n"
     ]
    }
   ],
   "source": [
    "tunning_fast_model_complet = fast.train_supervised(input='tuning_complet_aa', autotuneValidationFile='tuning_complet_ab', autotuneMetric=\"f1:__label__0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c722333",
   "metadata": {},
   "source": [
    "### Our final results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "4cb41a6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N\t25000\n",
      "P@1\t0.894\n",
      "R@1\t0.894\n"
     ]
    }
   ],
   "source": [
    "print_results(*tunning_fast_model_complet.test('lemmed_clean_imdb_test.txt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "389f2609",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label '__label__1':\n",
      "\n",
      "\tprecision: 0.897\n",
      "\trecall: nan\n",
      "\tF1 score: 1.795\n",
      "\n",
      "label '__label__0':\n",
      "\n",
      "\tprecision: 0.891\n",
      "\trecall: nan\n",
      "\tF1 score: 1.782\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_labels_results(tunning_fast_model_complet.test_label('lemmed_clean_imdb_test.txt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87ac6021",
   "metadata": {},
   "source": [
    "## Conclusion:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff6a3e17",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
