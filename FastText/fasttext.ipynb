{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9814cc89",
   "metadata": {},
   "source": [
    "# FastText and Word Vector (TP n°3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a85323ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "from nltk import tokenize\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "import spacy\n",
    "import fasttext as fast\n",
    "#import transformers\n",
    "\n",
    "from typing import Dict\n",
    "from typing import Callable\n",
    "from typing import List\n",
    "import re\n",
    "\n",
    "# set a defined random generator, better for reproducible results.\n",
    "random = np.random.default_rng(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49829ed2",
   "metadata": {},
   "source": [
    "## Take a look on IMDB dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c3efb039",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset imdb (/home/leherlemaxime/.cache/huggingface/datasets/imdb/plain_text/1.0.0/e3c66f1788a67a89c7058d97ff62b6c30531e05b549de56d3ab91891f0561f9a)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b98af7d6bbf142dc843ab2a51306bff2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 25000\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 25000\n",
      "    })\n",
      "    unsupervised: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 50000\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "imdb = load_dataset('imdb')\n",
    "print(imdb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87525b57",
   "metadata": {},
   "source": [
    "And we have the following number of entries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5d2e6ba5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train entries: 25000\n",
      "test entries: 25000\n"
     ]
    }
   ],
   "source": [
    "print(f\"train entries: {len(imdb['train'])}\\ntest entries: {len(imdb['test'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81f04a2f",
   "metadata": {},
   "source": [
    "## Translate dataset for FastText API:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1254f12f",
   "metadata": {},
   "source": [
    "Generate a shuffle index list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "198a8c87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 7615  7076  4795  2022 12317 22115 23608 20182 19417  1655]\n",
      "[23422  8434  2358  8980 15792  7655 20818 11901  1219 22105]\n"
     ]
    }
   ],
   "source": [
    "rand_idx = np.arange(len(imdb['train']))\n",
    "np.random.shuffle(rand_idx)\n",
    "print(rand_idx[:10])\n",
    "rand_idy = np.arange(len(imdb['train']))\n",
    "np.random.shuffle(rand_idy)\n",
    "print(rand_idy[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1f92ab7",
   "metadata": {},
   "source": [
    "Write IMDB dataset into file with FastText format:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bdaf3212",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 317 µs, sys: 89 µs, total: 406 µs\n",
      "Wall time: 251 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "if not os.path.exists(\"imdb_train.txt\"):\n",
    "    with open(\"imdb_train.txt\", \"wb\") as f:\n",
    "        for i in rand_idx:\n",
    "            entry = imdb['train'][int(i)]\n",
    "            s = f\"__label__{entry['label']} {entry['text']}\\n\".encode(\"utf-8\")\n",
    "            f.write(s)\n",
    "    \n",
    "        f.close()\n",
    "        \n",
    "if not os.path.exists(\"imdb_test.txt\"):\n",
    "    with open(\"imdb_test.txt\", \"wb\") as f:\n",
    "        for i in rand_idy:\n",
    "            entry = imdb['test'][int(i)]\n",
    "            s = f\"__label__{entry['label']} {entry['text']}\\n\".encode(\"utf-8\")\n",
    "            f.write(s)\n",
    "    \n",
    "        f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84d4c36c",
   "metadata": {},
   "source": [
    "Let's see the input format of an entry:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d322b72b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__label__0 Omen IV: The Awakening starts at the 'St. Frances Orphanage' where husband & wife Karen (Faye Grant) & Gene York (Michael Woods) are given a baby girl by Sister Yvonne (Megan Leitch) who they have adopted, they name her Delia. At first things go well but as the years pass & Delia (Asia Vieria) grows up Karen becomes suspicious of her as death & disaster follows her, Karen is convinced that she is evil itself. Karen then finds out that she is pregnant but discovers a sinister plot to use her as a surrogate mother for th next Antichrist & gets a shock when she finds out who Delia's real father was...<br /><br />Originally to be directed by Dominique Othenin-Girard who either quit or was sacked & was replaced by Jorge Montesi who completed the film although why he bothered is anyone's guess as Omen IV: The Awakening is absolutely terrible & a disgrace when compared to it illustrious predecessors. The script by Brian Taggert is hilariously bad, I'm not sure whether this nonsense actually looked good as the written word on a piece of paper but there are so many things wrong with it that I find even that hard to believe. As a serious film Omen IV: The AWakening falls flat on it's face & it really does work better if you look at it as a comedy spoof, I mean the scene towards the end when the Detective comes face-to-face with a bunch of zombie carol singers who are singing an ominous Gothic song has to be seen to be believed & I thought it was absolutely hilarious & ridiculous in equal measure. Then there's the pointless difference between this & the other Omen films in that this time it's a young girl, the question I ask here is why? Seriously, why? There's no reason at all & isn't used to any effect at all anyway. Then of course there's the stupid twist at the end which claims Delia has been keeping her brother's embryo inside herself & that in a sinister conspiracy involving a group of Satan worshippers it has been implanted in Karen so she can give birth to the Antichrist is moronic & comes across as just plain daft. At first it has a certain entertainment value in how bad it is but the unintentional hilarity gives way to complete boredom sooner rather than later.<br /><br />It's obviously impossible to know how much of Omen IV: The Awakening was directed by Girard & Montesi but you can sort of tell all was not well behind the camera as it's a shabby, cheap looking poorly made film which was actually made-for-TV & it shows with the bland, flat & unimaginative cinematography & production design. Then there's the total lack of scares, atmosphere, tension & gore which are the main elements that made the previous Omen films so effective.<br /><br />The budget must have been pretty low & the film looks like it was. The best most stylish thing about Omen IV: The Awakening is the final shot in which the camera rises up in the air as Delia walks away into the distance to reveal a crucifix shaped cross made by two overlapping path's but this is the very last shot before the end credits roll which says just about everything. I have to mention the music which sounds awful, more suited to a comedy & is very inappropriate sounding. The acting is alright at best but as usual the kid annoys.<br /><br />Omen IV: The Awakening is rubbish, it's a totally ridiculous film that tries to be serious & just ends up coming across as stupid. The change of director's probably didn't help either, that's still not a excuse though. The last Omen film to date following the original The Omen (1976), Damien: Omen II (1978) & The Final Conflict (1981) all of which are far superior to this.\r\n"
     ]
    }
   ],
   "source": [
    "!head -n 1 imdb_train.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17f8d696",
   "metadata": {},
   "source": [
    "## First training with FastText model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4ac30e68",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Read 5M words\n",
      "Number of words:  281132\n",
      "Number of labels: 2\n",
      "Progress: 100.0% words/sec/thread: 1312665 lr:  0.000000 avg.loss:  0.426153 ETA:   0h 0m 0s100.0% words/sec/thread: 1312778 lr: -0.000010 avg.loss:  0.426153 ETA:   0h 0m 0s\n"
     ]
    }
   ],
   "source": [
    "fast_model = fast.train_supervised('imdb_train.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "131052e8",
   "metadata": {},
   "source": [
    "Let's see the train vocabulary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5ed80c14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the vocabulary size is: 281132\n",
      "\n",
      "This is a slice of it:\n",
      "['the', 'a', 'and', 'of', 'to', 'is', 'in', 'I', 'that', 'this', 'it', '/><br', 'was', 'as', 'with', 'for', 'but', 'The', 'on', 'movie']\n"
     ]
    }
   ],
   "source": [
    "print(f\"the vocabulary size is: {len(fast_model.words)}\\n\\nThis is a slice of it:\\n{fast_model.words[:20]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a6fe6b2",
   "metadata": {},
   "source": [
    "### Results of the model:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50f10eb9",
   "metadata": {},
   "source": [
    "We respectfully copy and paste this print function from FastText documentation to see results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b1373799",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_results(N : int, p : float, r : float) -> None:\n",
    "    print(\"N\\t\" + str(N))\n",
    "    print(\"P@{}\\t{:.3f}\".format(1, p))\n",
    "    print(\"R@{}\\t{:.3f}\".format(1, r))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99f8ded3",
   "metadata": {},
   "source": [
    "So let's compute precision at 1 (P@1) and the recall on the test dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e202d23e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N\t25000\n",
      "P@1\t0.860\n",
      "R@1\t0.860\n"
     ]
    }
   ],
   "source": [
    "print_results(*fast_model.test('imdb_test.txt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "892b6996",
   "metadata": {},
   "source": [
    "And we can compute these metrics for all labels separately:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d975f76a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_labels_results(l_scores : Dict[str, Dict[str, float]]) -> None:\n",
    "    for label in l_scores:\n",
    "        print(f\"label '{label}':\\n\")\n",
    "        print(f\"\\tprecision: {np.round(l_scores[label]['precision'], 3)}\")\n",
    "        print(f\"\\trecall: {np.round(l_scores[label]['recall'], 3)}\")\n",
    "        print(f\"\\tF1 score: {np.round(l_scores[label]['f1score'], 3)}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d6d7b27d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label '__label__1':\n",
      "\n",
      "\tprecision: 0.856\n",
      "\trecall: nan\n",
      "\tF1 score: 1.713\n",
      "\n",
      "label '__label__0':\n",
      "\n",
      "\tprecision: 0.863\n",
      "\trecall: nan\n",
      "\tF1 score: 1.726\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_labels_results(fast_model.test_label('imdb_test.txt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ae963c3",
   "metadata": {},
   "source": [
    "## Pre-processing on IMDB dataset:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c41984f3",
   "metadata": {},
   "source": [
    "### Clean the text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e28a7db7",
   "metadata": {},
   "source": [
    "The text-format is not perfect, we have for exemple '\\t' or '<br\\>' that are formated text. So we will replace all special char by space. And we will also add space before and after '!' to make it a separated word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cfc1d9e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_the_text(text_array : str) -> str:\n",
    "    '''\n",
    "        This function return a list of all word and char in the text in parameters.\n",
    "\n",
    "            Parameters:\n",
    "                    text_array (str): The text in a string format.\n",
    "\n",
    "            Returns:\n",
    "                    result (str) : A list with all the word and char in the inpt text.\n",
    "    '''\n",
    "    \n",
    "    specialChars = \"()\\\\\\''.,;:\\\"?-\" \n",
    "    for specialChar in specialChars:\n",
    "        text_array = text_array.replace(specialChar, ' ')\n",
    "        \n",
    "    text_array = text_array.replace(\"/>\", ' ')\n",
    "    text_array = text_array.replace(\"<br\", ' ')\n",
    "    \n",
    "    ''' We add space before and after '!' for the split function '''\n",
    "    text_array = text_array.replace(\"!\", \" ! \")\n",
    "    \n",
    "    return text_array.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b323835",
   "metadata": {},
   "source": [
    "Now we can try the same model but with the clean text and see if this modification change the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "98c92795",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 402 µs, sys: 36 µs, total: 438 µs\n",
      "Wall time: 242 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "if not os.path.exists(\"imdb_clean_train.txt\"):\n",
    "    with open(\"imdb_clean_train.txt\", \"wb\") as f:\n",
    "        for i in rand_idx:\n",
    "            entry = imdb['train'][int(i)]\n",
    "            s = f\"__label__{entry['label']} {clean_the_text(entry['text'])}\\n\".encode(\"utf-8\")\n",
    "            f.write(s)\n",
    "    \n",
    "        f.close()\n",
    "        \n",
    "if not os.path.exists(\"imdb_clean_test.txt\"):\n",
    "    with open(\"imdb_clean_test.txt\", \"wb\") as f:\n",
    "        for i in rand_idy:\n",
    "            entry = imdb['test'][int(i)]\n",
    "            s = f\"__label__{entry['label']} {clean_the_text(entry['text'])}\\n\".encode(\"utf-8\")\n",
    "            f.write(s)\n",
    "    \n",
    "        f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "210d0157",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__label__0 what a time we live in when someone like this joe swan whatever the hell is considered a good filmmaker   or even a filmmaker at all !  where are the new crop of filmmakers with brains and talent    we need them bad  and to hell with mumblecore !       this movie is about nothing  just as the characters in the film stand for nothing  it s this horrible  so called gen y  that is full of bored idiots  some of which declare themselves filmmakers with out bothering to learn anything about the craft before shooting  well  orson welles was a filmmaker  john huston was a filmmaker  fellini was a filmmaker  dreyer was a filmmaker  etc  current films like these show just how stupid young  so called  filmmakers  can be when they believe going out with no script  no direction  no thought  no legit  camerawork   everything shot horribly on dv   no craft of editing  no nothing  stands for  rebellious  or  advanced  film making  nope  it s called ignorance and laziness or just pure masturbation of cinema  and there actually is an in your face  jack off shot   so be ready         look at the early films of any accomplished  indie  filmmaker  linklatter  morris  allen  lynch  hartley  jarmusch  jost  lee  or herzog   none made anything as tedious and aimless as this  yet swan whatever the hell  is still going to sxsw every year and hailed as some kind of gutsy  new talent  it s crap !  i can t imagine anyone liking this  and everything else this so called filmmaker has done  all seen by me  is just as bad  the newer stuff clearly made to appeal to a more mainstream audience  one of the sitcom calling   steer clear  unless you re a friend or family member of those involved   on second thought  if you re a family member or friend you d probably be embarrassed to see a family member or friend in such compromising situations         utter garbage  this isn t art  this is the ultimate opposite of it \r\n"
     ]
    }
   ],
   "source": [
    "!head -n 1 imdb_clean_train.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6d5ff022",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Read 6M words\n",
      "Number of words:  80799\n",
      "Number of labels: 2\n",
      "Progress: 100.0% words/sec/thread: 1572989 lr:  0.000000 avg.loss:  0.389734 ETA:   0h 0m 0s\n"
     ]
    }
   ],
   "source": [
    "fast_model_clean = fast.train_supervised('imdb_clean_train.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f13e1733",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the vocabulary size is: 80799\n",
      "\n",
      "This is a slice of it:\n",
      "['the', 'and', 'a', 'of', 'to', 'is', 'it', 'in', 'i', 'this', 'that', 's', 'was', 'as', 'for', 'with', 'movie', 'but', 'film', 'you']\n"
     ]
    }
   ],
   "source": [
    "print(f\"the vocabulary size is: {len(fast_model_clean.words)}\\n\\nThis is a slice of it:\\n{fast_model_clean.words[:20]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9647f111",
   "metadata": {},
   "source": [
    "#### Result of clean model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fe13767c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N\t25000\n",
      "P@1\t0.879\n",
      "R@1\t0.879\n"
     ]
    }
   ],
   "source": [
    "print_results(*fast_model_clean.test('imdb_clean_test.txt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e60ed600",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label '__label__0':\n",
      "\n",
      "\tprecision: 0.878\n",
      "\trecall: nan\n",
      "\tF1 score: 1.757\n",
      "\n",
      "label '__label__1':\n",
      "\n",
      "\tprecision: 0.879\n",
      "\trecall: nan\n",
      "\tF1 score: 1.758\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_labels_results(fast_model_clean.test_label('imdb_clean_test.txt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3549de4",
   "metadata": {},
   "source": [
    "So we can see that in average we have a upgrade of our result of 0.02. It's not a huge upgrade but it's still ok for a few more seconds of calculation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3e7b9c9",
   "metadata": {},
   "source": [
    "### Clean text with stop word"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35b3154a",
   "metadata": {},
   "source": [
    "We have see that the function to clean upgrade our result but why stop here ?\n",
    "\n",
    "We can add a other clean step on the text, this step is to delete stop words. What is stop words ? Stop words are the non-discriminating words, like __the__, __a__, __an__, __this__ ....\n",
    "\n",
    "So first of all we will create a list of all the stop word and after we will delete them from text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "99de6205",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_stop_word = [\"the\", \"and\", \"a\", \"of\", \"to\", \"is\", \"it\", \"in\", \"this\", \"that\", \"s\", \"was\", \"as\", \"for\", \"with\", \"but\", \"then\", \"an\", \"at\", \"who\", \"when\", \"than\", \"where\", \"which\", \"with\", \"on\", \"t\", \"are\", \"by\", \"so\", \"from\", \"have\", \"be\", \"or\", \"just\", \"about\", \"\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a81e3a4",
   "metadata": {},
   "source": [
    "Now we will create our extend clean text function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3354de88",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_the_text_extend(text : str, list_of_stop_word : List[str]) -> str:\n",
    "    '''\n",
    "        This function return a list of all word and char in the text in parameters.\n",
    "\n",
    "            Parameters:\n",
    "                    text (str): The text in a string format.\n",
    "                    \n",
    "                    list_of_stop_word: This is that list of our stop words to remove from the text\n",
    "\n",
    "            Returns:\n",
    "                    result (str) : A list with all the word and char in the inpt text.\n",
    "    '''\n",
    "    text = text.lower()\n",
    "    \n",
    "    specialChars = \"()\\\\\\''.,;:\\\"?-\" \n",
    "    for specialChar in specialChars:\n",
    "        text = text.replace(specialChar, ' ')\n",
    "        \n",
    "    text = text.replace(\"/>\", ' ')\n",
    "    text = text.replace(\"<br\", ' ')\n",
    "    \n",
    "    text = text.replace(\"</s>\", \" \")\n",
    "    \n",
    "    ''' We add space before and after '!' for the split function '''\n",
    "    text = text.replace(\"!\", \" ! \")\n",
    "    \n",
    "    for word in list_of_stop_word:\n",
    "        ''' We add this to only remove the all word and not isolated letter in an other word'''\n",
    "        word = \" \" + word + \" \"\n",
    "        text = text.replace(word, \" \")\n",
    "    \n",
    "    return text.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e3d8d45",
   "metadata": {},
   "source": [
    "Now try again with this new function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b7446453",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 635 µs, sys: 40 µs, total: 675 µs\n",
      "Wall time: 427 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "if not os.path.exists(\"imdb_clean_extend_train.txt\"):\n",
    "    with open(\"imdb_clean_extend_train.txt\", \"wb\") as f:\n",
    "        for i in rand_idx:\n",
    "            entry = imdb['train'][int(i)]\n",
    "            s = f\"__label__{entry['label']} {clean_the_text_extend(entry['text'], list_of_stop_word)}\\n\".encode(\"utf-8\")\n",
    "            f.write(s)\n",
    "    \n",
    "        f.close()\n",
    "\n",
    "if not os.path.exists(\"imdb_clean_extend_test.txt\"):\n",
    "    with open(\"imdb_clean_extend_test.txt\", \"wb\") as f:\n",
    "        for i in rand_idy:\n",
    "            entry = imdb['test'][int(i)]\n",
    "            s = f\"__label__{entry['label']} {clean_the_text_extend(entry['text'], list_of_stop_word)}\\n\".encode(\"utf-8\")\n",
    "            f.write(s)\n",
    "    \n",
    "        f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "802cc364",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__label__0 this yawn titles credits  boring point tedium acting wooden stilted !  admittedly director richard jobson directing debut  earth green lit script poorly developed one  looks like another money down drain government project  scottish screen credited surprise  surprise   nearly fell asleep three times my review will unfortunately more restrained one  please  please mister jobson what ever you ve been doing prior directing sedative  go back ! \r\n"
     ]
    }
   ],
   "source": [
    "!head -n 1 imdb_clean_extend_train.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ad6ae041",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Read 3M words\n",
      "Number of words:  80799\n",
      "Number of labels: 2\n",
      "Progress: 100.0% words/sec/thread: 1550060 lr:  0.000000 avg.loss:  0.323175 ETA:   0h 0m 0s\n"
     ]
    }
   ],
   "source": [
    "fast_model_clean_extend = fast.train_supervised('imdb_clean_extend_train.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4c9a2f03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the vocabulary size is: 80799\n",
      "\n",
      "This is a slice of it:\n",
      "['you', 'not', 'one', '</s>', '!', 'all', 'they', 'like', 'there', 'or', 'just', 'about', 'out', 'if', 'has', 'what', 'some', 'good', 'can', 'more']\n"
     ]
    }
   ],
   "source": [
    "print(f\"the vocabulary size is: {len(fast_model_clean_extend.words)}\\n\\nThis is a slice of it:\\n{fast_model_clean_extend.words[:20]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21e53bc3",
   "metadata": {},
   "source": [
    "#### Result of clean model extend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "da8ee6ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N\t25000\n",
      "P@1\t0.885\n",
      "R@1\t0.885\n"
     ]
    }
   ],
   "source": [
    "print_results(*fast_model_clean_extend.test('imdb_clean_extend_test.txt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "bbbee235",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label '__label__1':\n",
      "\n",
      "\tprecision: 0.888\n",
      "\trecall: nan\n",
      "\tF1 score: 1.775\n",
      "\n",
      "label '__label__0':\n",
      "\n",
      "\tprecision: 0.883\n",
      "\trecall: nan\n",
      "\tF1 score: 1.766\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_labels_results(fast_model_clean_extend.test_label('imdb_clean_extend_test.txt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82194d0c",
   "metadata": {},
   "source": [
    "So with this result we can see that the result are abit better so we will now use the clean text expand instead of clean text classic."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60794076",
   "metadata": {},
   "source": [
    "### Stemming the data:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7b643a8",
   "metadata": {},
   "source": [
    "First of all we need to create a function that stemme a word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "77e4fac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "re_word = re.compile(r\"^\\w+$\")\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "\n",
    "stem_word: Callable[[str], str] = lambda w : stemmer.stem(w.lower()) if re_word.match(w) else w"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "545dd43c",
   "metadata": {},
   "source": [
    "Now we have ti create a function that apply stemming to a whole text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4f981507",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stemming_text(text : str) -> str:\n",
    "    '''\n",
    "        This function steeming the text in parameter and return in\n",
    "        \n",
    "        Parameters :\n",
    "                text (str) : the text to stemming\n",
    "                \n",
    "        Returns :\n",
    "                return_text (str) : the text stemmed\n",
    "    '''\n",
    "    list_of_words = text.split(\" \")\n",
    "    \n",
    "    list_of_words = [stem_word(word) for word in list_of_words]\n",
    "    \n",
    "    return_text = \" \".join(list_of_words)\n",
    "    \n",
    "    return return_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08f2d1c6",
   "metadata": {},
   "source": [
    "Now with this function we can create the new model where we use the stemming for all text before write them in the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a3da7b59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 140 µs, sys: 7 µs, total: 147 µs\n",
      "Wall time: 155 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "if not os.path.exists(\"imdb_stemmed_train.txt\"):\n",
    "    with open(\"imdb_stemmed_train.txt\", \"wb\") as f:\n",
    "        for i in rand_idx:\n",
    "            entry = imdb['train'][int(i)]\n",
    "            s = f\"__label__{entry['label']} {stemming_text(entry['text'])}\\n\".encode(\"utf-8\")\n",
    "            f.write(s)\n",
    "    \n",
    "        f.close()\n",
    "        \n",
    "if not os.path.exists(\"imdb_stemmed_test.txt\"):\n",
    "    with open(\"imdb_stemmed_test.txt\", \"wb\") as f:\n",
    "        for i in rand_idy:\n",
    "            entry = imdb['test'][int(i)]\n",
    "            s = f\"__label__{entry['label']} {stemming_text(entry['text'])}\\n\".encode(\"utf-8\")\n",
    "            f.write(s)\n",
    "    \n",
    "        f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b2a41939",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__label__1 i watch this last night after not have seen it for sever years. it realli is a fun littl film, with a bunch of face you didn't know were in it. arkin shine as always. check it out; you won't be dissappointed. by the way, it was just releas on dvd and contrari to it packaging, it is widescreen. the transfer is rather poor, but at least the whole movi is visible. ;-)\r\n"
     ]
    }
   ],
   "source": [
    "!head -n 1 imdb_stemmed_train.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e596fd3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Read 5M words\n",
      "Number of words:  245430\n",
      "Number of labels: 2\n",
      "Progress: 100.0% words/sec/thread: 1497238 lr:  0.000000 avg.loss:  0.419089 ETA:   0h 0m 0s100.0% words/sec/thread: 1497419 lr: -0.000007 avg.loss:  0.419089 ETA:   0h 0m 0s\n"
     ]
    }
   ],
   "source": [
    "fast_model_stemmed = fast.train_supervised('imdb_stemmed_train.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e79a8d61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the vocabulary size is: 245430\n",
      "\n",
      "This is a slice of it:\n",
      "['the', 'a', 'and', 'of', 'to', 'is', 'in', 'it', 'i', 'this', 'that', '/><br', 'was', 'as', 'for', 'with', 'but', 'movi', 'film', 'be']\n"
     ]
    }
   ],
   "source": [
    "print(f\"the vocabulary size is: {len(fast_model_stemmed.words)}\\n\\nThis is a slice of it:\\n{fast_model_stemmed.words[:20]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48116788",
   "metadata": {},
   "source": [
    "#### Result of stemmed model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "bbdcb864",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N\t25000\n",
      "P@1\t0.861\n",
      "R@1\t0.861\n"
     ]
    }
   ],
   "source": [
    "print_results(*fast_model_stemmed.test('imdb_stemmed_test.txt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "8a8cd6e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label '__label__1':\n",
      "\n",
      "\tprecision: 0.853\n",
      "\trecall: nan\n",
      "\tF1 score: 1.705\n",
      "\n",
      "label '__label__0':\n",
      "\n",
      "\tprecision: 0.87\n",
      "\trecall: nan\n",
      "\tF1 score: 1.739\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_labels_results(fast_model_stemmed.test_label('imdb_stemmed_test.txt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d283b8c0",
   "metadata": {},
   "source": [
    "The stemmed model don't change the result or juste of 0.002 in the label 1 so the result is not convincing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27ee55bb",
   "metadata": {},
   "source": [
    "### Lemming the data:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67dfb962",
   "metadata": {},
   "source": [
    "Firstly, we need to download the english model of Spacy lemmatization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "50012030",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m spacy download en_core_web_sm > output_dl.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "5bc8baf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading the small English model\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "152368f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 213 µs, sys: 10 µs, total: 223 µs\n",
      "Wall time: 251 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "if not os.path.exists(\"lemmed_imdb_train.txt\"):\n",
    "    with open(\"lemmed_imdb_train.txt\", \"wb\") as f:\n",
    "        for i in rand_idx:\n",
    "            entry = imdb['train'][int(i)]\n",
    "            \n",
    "            # lemmatize before writting\n",
    "            lemmed_text = ' '.join([token.lemma_ for token in nlp(entry['text'])])\n",
    "            s = f\"__label__{entry['label']} {lemmed_text}\\n\".encode(\"utf-8\")\n",
    "            f.write(s)\n",
    "    \n",
    "        f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb82b8a6",
   "metadata": {},
   "source": [
    "Do it in test dataset also:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "22937706",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(\"lemmed_imdb_test.txt\"):\n",
    "    with open(\"lemmed_imdb_test.txt\", \"wb\") as f:\n",
    "        for i in rand_idy:\n",
    "            entry = imdb['test'][int(i)]\n",
    "            \n",
    "            # lemmatize before writting\n",
    "            lemmed_text = ' '.join([token.lemma_ for token in nlp(entry['text'])])\n",
    "            s = f\"__label__{entry['label']} {lemmed_text}\\n\".encode(\"utf-8\")\n",
    "            f.write(s)\n",
    "    \n",
    "        f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "8acdb4b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Read 6M words\n",
      "Number of words:  106199\n",
      "Number of labels: 2\n",
      "Progress: 100.0% words/sec/thread: 1511882 lr:  0.000000 avg.loss:  0.417234 ETA:   0h 0m 0s\n"
     ]
    }
   ],
   "source": [
    "fast_model_lemming = fast.train_supervised('lemmed_imdb_train.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "6d87da66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the vocabulary size is: 106199\n",
      "\n",
      "This is a slice of it:\n",
      "['the', 'be', ',', '.', 'and', 'a', 'of', 'to', 'it', 'I', 'in', 'this', 'that', '\"', 'have', '-', '/><br', 'movie', 'film', 'as']\n"
     ]
    }
   ],
   "source": [
    "print(f\"the vocabulary size is: {len(fast_model_lemming.words)}\\n\\nThis is a slice of it:\\n{fast_model_lemming.words[:20]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8793d9e0",
   "metadata": {},
   "source": [
    "#### Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "e5875f31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N\t25000\n",
      "P@1\t0.867\n",
      "R@1\t0.867\n"
     ]
    }
   ],
   "source": [
    "print_results(*fast_model_lemming.test('lemmed_imdb_test.txt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "5881726e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label '__label__0':\n",
      "\n",
      "\tprecision: 0.869\n",
      "\trecall: nan\n",
      "\tF1 score: 1.739\n",
      "\n",
      "label '__label__1':\n",
      "\n",
      "\tprecision: 0.865\n",
      "\trecall: nan\n",
      "\tF1 score: 1.729\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_labels_results(fast_model_lemming.test_label('lemmed_imdb_test.txt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4368f4ed",
   "metadata": {},
   "source": [
    "We can see that this time the improvement is a little more interesting, even if it is very long (about 1 hour on the computer where we did the test). But we will keep it because it is quite long but it allows an improvement and we will see how it couples with the other optimization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7404829f",
   "metadata": {},
   "source": [
    "## Hyperparameters tunning:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf2a357e",
   "metadata": {},
   "source": [
    "We need to extract a validation set of our train dataset to avoid a tunning validation on test dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "02366b7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-r--r-- 1 leherlemaxime leherlemaxime 26719815 Oct  6 16:57 ./tuning_aa\r\n",
      "-rw-r--r-- 1 leherlemaxime leherlemaxime  6713008 Oct  6 16:57 ./tuning_ab\r\n",
      "-rw-r--r-- 1 leherlemaxime leherlemaxime 19853802 Oct  6 16:48 ./tuning_complet_aa\r\n",
      "-rw-r--r-- 1 leherlemaxime leherlemaxime  4974531 Oct  6 16:48 ./tuning_complet_ab\r\n"
     ]
    }
   ],
   "source": [
    "# split command will copy and separate file into set of files of 20000 lines.\n",
    "# Train file have 25.000 lines, so train will have 20.000 lines and validation 5.000 lines.\n",
    "!split -l20000 \"imdb_train.txt\" tuning_\n",
    "\n",
    "!ls -l ./tuning_*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e26d837",
   "metadata": {},
   "source": [
    "### Try the default hyperparameter tunning of FastText:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "3f315ad8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Progress: 100.0% Trials:    9 Best score:  0.884967 ETA:   0h 0m 0s\n",
      "Training again with best arguments\n",
      "Read 4M words\n",
      "Number of words:  244423\n",
      "Number of labels: 2\n",
      "Progress: 100.0% words/sec/thread:  603064 lr:  0.000000 avg.loss:  0.048096 ETA:   0h 0m 0s100.0% words/sec/thread:  603065 lr: -0.000001 avg.loss:  0.048096 ETA:   0h 0m 0s\n"
     ]
    }
   ],
   "source": [
    "tunning_fast_model = fast.train_supervised(input='tuning_aa', autotuneValidationFile='tuning_ab', autotuneMetric=\"f1:__label__0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72fcbfef",
   "metadata": {},
   "source": [
    "Let's compute global metrics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "b0845727",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N\t25000\n",
      "P@1\t0.883\n",
      "R@1\t0.883\n"
     ]
    }
   ],
   "source": [
    "print_results(*tunning_fast_model.test('imdb_test.txt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69d8a41a",
   "metadata": {},
   "source": [
    "It looks to give better results with default hyperparameter tunning. But how labels scores change ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "65768679",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label '__label__1':\n",
      "\n",
      "\tprecision: 0.882\n",
      "\trecall: nan\n",
      "\tF1 score: 1.765\n",
      "\n",
      "label '__label__0':\n",
      "\n",
      "\tprecision: 0.884\n",
      "\trecall: nan\n",
      "\tF1 score: 1.768\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_labels_results(tunning_fast_model.test_label('imdb_test.txt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8508810",
   "metadata": {},
   "source": [
    "Results are better with tunning and we highlight that optimize f1 result on __negative__ label induces better improvements on __positive__ label. The reason is because we juste have two labels and __negative__ label had less wrongly classification."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af29af9e",
   "metadata": {},
   "source": [
    "## Merge optimisation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a6f8061",
   "metadata": {},
   "source": [
    "First we will try to add the clean text extend optimisation to other optimisaion because we see that this optimisation clean the text and just keep this important part. We also see that lemming is much better that stemming.\n",
    "\n",
    "It's the reason why we try a model with clean text extend and lemming."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "71fc2195",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 6.48 ms, sys: 0 ns, total: 6.48 ms\n",
      "Wall time: 6.01 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "if not os.path.exists(\"lemmed_clean_imdb_train.txt\"):\n",
    "    with open(\"lemmed_clean_imdb_train.txt\", \"wb\") as f:\n",
    "        for i in rand_idx:\n",
    "            entry = imdb['train'][int(i)]\n",
    "            \n",
    "            # lemmatize before writting\n",
    "            lemmed_text = ' '.join([token.lemma_ for token in nlp(clean_the_text_extend(entry['text'], list_of_stop_word))])\n",
    "            s = f\"__label__{entry['label']} {lemmed_text}\\n\".encode(\"utf-8\")\n",
    "            f.write(s)\n",
    "    \n",
    "        f.close()\n",
    "\n",
    "if not os.path.exists(\"lemmed_clean_imdb_test.txt\"):\n",
    "    with open(\"lemmed_clean_imdb_test.txt\", \"wb\") as f:\n",
    "        for i in rand_idy:\n",
    "            entry = imdb['test'][int(i)]\n",
    "            \n",
    "            # lemmatize before writting\n",
    "            lemmed_text = ' '.join([token.lemma_ for token in nlp(clean_the_text_extend(entry['text'], list_of_stop_word))])\n",
    "            s = f\"__label__{entry['label']} {lemmed_text}\\n\".encode(\"utf-8\")\n",
    "            f.write(s)\n",
    "    \n",
    "        f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "2f641576",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Read 3M words\n",
      "Number of words:  64059\n",
      "Number of labels: 2\n",
      "Progress: 100.0% words/sec/thread: 1563884 lr:  0.000000 avg.loss:  0.326712 ETA:   0h 0m 0s\n"
     ]
    }
   ],
   "source": [
    "fast_model_lemming_clean = fast.train_supervised('lemmed_clean_imdb_train.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "f46477f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the vocabulary size is: 64059\n",
      "\n",
      "This is a slice of it:\n",
      "['you', 'not', 'they', 'have', 'be', 'one', 'do', '</s>', '!', 'all', 'see', 'make', 'like', 'good', 'there', 'well', 'or', 'just', 'about', 'out']\n"
     ]
    }
   ],
   "source": [
    "print(f\"the vocabulary size is: {len(fast_model_lemming_clean.words)}\\n\\nThis is a slice of it:\\n{fast_model_lemming_clean.words[:20]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42602797",
   "metadata": {},
   "source": [
    "#### Results of combine optimisations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "7e54b70c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N\t25000\n",
      "P@1\t0.880\n",
      "R@1\t0.880\n"
     ]
    }
   ],
   "source": [
    "print_results(*fast_model_lemming_clean.test('lemmed_clean_imdb_test.txt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "5701fde8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label '__label__0':\n",
      "\n",
      "\tprecision: 0.874\n",
      "\trecall: nan\n",
      "\tF1 score: 1.747\n",
      "\n",
      "label '__label__1':\n",
      "\n",
      "\tprecision: 0.886\n",
      "\trecall: nan\n",
      "\tF1 score: 1.772\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_labels_results(fast_model_lemming_clean.test_label('lemmed_clean_imdb_test.txt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46872a8c",
   "metadata": {},
   "source": [
    "So with these 2 optimisation we have not a improvement compare to clean text extended only. OUr hypothesys is that we start to overfit on the train set. So a solution is to use this methode but with tunning hyperparametres."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee5fd9be",
   "metadata": {},
   "source": [
    "## Our final model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "f94cbb63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-r--r-- 1 leherlemaxime leherlemaxime 19853802 Oct  6 16:48 ./tuning_complet_aa\r\n",
      "-rw-r--r-- 1 leherlemaxime leherlemaxime  4974531 Oct  6 16:48 ./tuning_complet_ab\r\n"
     ]
    }
   ],
   "source": [
    "# split command will copy and separate file into set of files of 20000 lines.\n",
    "# Train file have 25.000 lines, so train will have 20.000 lines and validation 5.000 lines.\n",
    "!split -l20000 \"lemmed_clean_imdb_train.txt\" tuning_complet_\n",
    "\n",
    "!ls -l ./tuning_complet_*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a376a6f7",
   "metadata": {},
   "source": [
    "We have manually work on the parameter epochs, rate and n-grams. But we don't succed in find a better resulat that juste use autometric so we juste use this to get our better result on this data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "8a0ff0e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Progress: 100.0% Trials:    9 Best score:  0.890872 ETA:   0h 0m 0s\n",
      "Training again with best arguments\n",
      "Read 3M words\n",
      "Number of words:  58103\n",
      "Number of labels: 2\n",
      "Progress: 100.0% words/sec/thread:  406096 lr:  0.000000 avg.loss:  0.035149 ETA:   0h 0m 0s\n"
     ]
    }
   ],
   "source": [
    "tunning_fast_model_complet = fast.train_supervised(input='tuning_complet_aa', autotuneValidationFile='tuning_complet_ab', autotuneMetric=\"f1:__label__0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c722333",
   "metadata": {},
   "source": [
    "### Our final results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "4cb41a6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N\t25000\n",
      "P@1\t0.894\n",
      "R@1\t0.894\n"
     ]
    }
   ],
   "source": [
    "print_results(*tunning_fast_model_complet.test('lemmed_clean_imdb_test.txt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "389f2609",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label '__label__1':\n",
      "\n",
      "\tprecision: 0.897\n",
      "\trecall: nan\n",
      "\tF1 score: 1.795\n",
      "\n",
      "label '__label__0':\n",
      "\n",
      "\tprecision: 0.892\n",
      "\trecall: nan\n",
      "\tF1 score: 1.783\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_labels_results(tunning_fast_model_complet.test_label('lemmed_clean_imdb_test.txt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87ac6021",
   "metadata": {},
   "source": [
    "## Conclusion:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff6a3e17",
   "metadata": {},
   "source": [
    "So we arrive with our final model to really satisfactory results for what we wanted we get close to 90% accuracy on the 2 labels. Even if this model is quite long (about 1h20 to do everything) we find that it is a pretty good time for this result. But we will see examples of misclassified texts to try to understand why they are misclassified and see the weakness of our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "ff132b43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label '__label__1':\n",
      "\n",
      "\tprecision: 0.897\n",
      "\trecall: nan\n",
      "\tF1 score: 1.795\n",
      "\n",
      "label '__label__0':\n",
      "\n",
      "\tprecision: 0.892\n",
      "\trecall: nan\n",
      "\tF1 score: 1.783\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_labels_results(tunning_fast_model_complet.test_label('lemmed_clean_imdb_test.txt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "2a6f91fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "wrong_0_index = []\n",
    "wrong_1_index = []\n",
    "\n",
    "for i in range(len(imdb[\"test\"])):\n",
    "    label_predict = tunning_fast_model_complet.predict(imdb[\"test\"][i][\"text\"])[0][0]\n",
    "    if ((label_predict == \"__label__1\") and (imdb[\"test\"][i][\"label\"] == 0)):\n",
    "        wrong_1_index.append(i)\n",
    "    elif ((label_predict == \"__label__0\") and (imdb[\"test\"][i][\"label\"] == 1)):\n",
    "        wrong_0_index.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "a5c17bca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "717"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(wrong_0_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "2dd01755",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5202"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(wrong_1_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afc3b240",
   "metadata": {},
   "source": [
    "2 examples of wrong 0 label :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "e138c449",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"black tar can't be snorted there's a documentary: dark end of the street about s.f. street punks and b.t. abuse - not bad - quite heavy. in wasted there's this stuff that looks like coke but should be something else... no big deal. black tar can't be snorted there's a documentary: dark end of the street about s.f. street punks and b.t. abuse - not bad - quite heavy. in wasted there's this stuff that looks like coke but should be something else... no big deal. black tar can't be snorted there's a documentary: dark end of the street about s.f. street punks and b.t. abuse - not bad - quite heavy. in wasted there's this stuff that looks like coke but should be something else... no big deal.\""
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imdb[\"test\"][int(random.choice(wrong_0_index))][\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "4c53fd7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I loved this film! Fantastically original and different! A solid, intense, hard-core and suspenseful movie that has just the right touch of (dark?) humor. If you're tired of the typical, overdone, ridiculous Hollywood B.S. movies, how many big explosions and awful and unrealistic shoot em up gun fights that insult your intelligence can we take, then this film is for you. Fantastic characters that are wonderfully original and believable, and solid performances by all actors, not a weak character or performance in the film. Skip Woods' film is a breath of fresh air and I applaud his originality and efforts, his film has the feel of a cross between a Quentin Tarantino and a Cohen brothers film (not a bad mix at all in my opinion). This movie grabs you by the throat and doesn't let go, there's nothing boring or bubble gum about this film. The only disappointment is that nobody seems to know about it, everyone I've recommended it to has thanked me and shared my opinion on it. This film is a welcomed change/alternative from the canned Hollywood mainstream garbage being produced today, even with their big name actors, big explosions, special effects and huge budgets. It's a terrifically wild, intense, violent, graphic, humorous and raw (I mean that in a good way, no phony Hollywood polish here) ride. Thank you to everyone involved in making this film happen, you did an incredible job!\""
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imdb[\"test\"][int(random.choice(wrong_0_index))][\"text\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39280e48",
   "metadata": {},
   "source": [
    "2 examples of wrong 1 label :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "49cae33b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I haven\\'t seen this film in years, but the awful \"taste\" of Quaid\\'s performance still lingers on my tongue. Some have commented on how Quaid has Jerry Lee Lewis \"to a tee\" but the fact is he only appears to have the most extreme stage Jerry in mind. Nobody acts that way all the time, and the performance comes off as hopelessly clownish, reducing Lewis to a buffoonish caricature. The nuances of a man\\'s life are lost in the rubble of sheer over-acting.<br /><br />The author of the book this is based on (Nick Tosches) is a good writer, who has written several fine musical bios (I particularly liked \"Dino\" on Dean Martin); in the books Tosches gives us a full human being, both separate from and involved in the \"biz.\" Quaid\\'s acting seems to imply that Jerry never acted like a human being. If people were like this, no one would bother to hang around them. As cartoons go, it is mildly amusing, but otherwise it is one of the most egregious, film-destroying performances I have had the \"honor\" of viewing. Terrible...'"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imdb[\"test\"][int(random.choice(wrong_1_index))][\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "b591f697",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"This is a movie with an excellent concept for a story but that got sidetracked but a large number of clichéd sub-plots, hackneyed and unrealistic portrayed characterizations and performances, and some frankly implausible (and highly coincidental and, not to mention, convenient as plot points to move the story to its inexorable finish).<br /><br />The lack of anything that marked the lead as actually gay, other than some coincidental references to Crow Bar or that he's gay, was troubling. It wouldn't have hurt to actually show him do something, even if it was just meet a friend for drinks.<br /><br />It's worth checking out and has it's merits. There isn't much, even now a few years after the movie was released, in the way of movies that feature both a lead that is gay, or a significant gay plot line, and that is also about African-Americans. For that, it's worth checking out. I wouldn't look too hard for it and I wouldn't waste my time looking for it to own. This is a rental, and not a premium rental at that.\""
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imdb[\"test\"][int(random.choice(wrong_1_index))][\"text\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c0083d6",
   "metadata": {},
   "source": [
    "After analyzing some examples, the previous ones and others not shown here, we understand in fact the problem of our model on the 10% error. Indeed it does not take into account the context. Indeed a person can say that a movie is good because it is different from other movies, but if he says that there is a strong chance that this person will give a negative feeling on the other movies he is talking about and so as our classifier does not take into account the context he will think that these negative adjectives are used for the movie. And the second problem of this contextualization can be seen with expressions with negative connotations but used in a positive context for example \"with an extremely dark humor\", this is not necessarily negative but as the terms used are we arrive at a classification problem.\n",
    "\n",
    "So to summarize our model has a very good success rate and the big problem that can remain to solve is the context handle but which is much more complicated."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
