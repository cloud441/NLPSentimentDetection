{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9814cc89",
   "metadata": {},
   "source": [
    "# FastText and Word Vector (TP n°3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a85323ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "from nltk import tokenize\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "import spacy\n",
    "import fasttext as fast\n",
    "#import transformers\n",
    "\n",
    "from typing import Dict\n",
    "from typing import Callable\n",
    "from typing import List\n",
    "import re\n",
    "\n",
    "# set a defined random generator, better for reproducible results.\n",
    "random = np.random.default_rng(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49829ed2",
   "metadata": {},
   "source": [
    "## Take a look on IMDB dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c3efb039",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset imdb (/home/leherlemaxime/.cache/huggingface/datasets/imdb/plain_text/1.0.0/e3c66f1788a67a89c7058d97ff62b6c30531e05b549de56d3ab91891f0561f9a)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e65793746d314b9096ce6542f5491cc4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 25000\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 25000\n",
      "    })\n",
      "    unsupervised: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 50000\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "imdb = load_dataset('imdb')\n",
    "print(imdb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87525b57",
   "metadata": {},
   "source": [
    "And we have the following number of entries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5d2e6ba5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train entries: 25000\n",
      "test entries: 25000\n"
     ]
    }
   ],
   "source": [
    "print(f\"train entries: {len(imdb['train'])}\\ntest entries: {len(imdb['test'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81f04a2f",
   "metadata": {},
   "source": [
    "## Translate dataset for FastText API:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1254f12f",
   "metadata": {},
   "source": [
    "Generate a shuffle index list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "198a8c87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 6401 19017  6643 17772 19755 23333 15152 18940 14600 14428]\n"
     ]
    }
   ],
   "source": [
    "rand_idx = np.arange(len(imdb['train']))\n",
    "np.random.shuffle(rand_idx)\n",
    "print(rand_idx[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1f92ab7",
   "metadata": {},
   "source": [
    "Write IMDB dataset into file with FastText format:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bdaf3212",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 461 µs, sys: 167 µs, total: 628 µs\n",
      "Wall time: 529 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "if not os.path.exists(\"imdb_train.txt\"):\n",
    "    with open(\"imdb_train.txt\", \"wb\") as f:\n",
    "        for i in rand_idx:\n",
    "            entry = imdb['train'][int(i)]\n",
    "            s = f\"__label__{entry['label']} {entry['text']}\\n\".encode(\"utf-8\")\n",
    "            f.write(s)\n",
    "    \n",
    "        f.close()\n",
    "        \n",
    "# We shuffle rand_idx to apply with test dataset ...\n",
    "np.random.shuffle(rand_idx)\n",
    "        \n",
    "if not os.path.exists(\"imdb_test.txt\"):\n",
    "    with open(\"imdb_test.txt\", \"wb\") as f:\n",
    "        for i in rand_idx:\n",
    "            entry = imdb['test'][int(i)]\n",
    "            s = f\"__label__{entry['label']} {entry['text']}\\n\".encode(\"utf-8\")\n",
    "            f.write(s)\n",
    "    \n",
    "        f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84d4c36c",
   "metadata": {},
   "source": [
    "Let's see the input format of an entry:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d322b72b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__label__0 Omen IV: The Awakening starts at the 'St. Frances Orphanage' where husband & wife Karen (Faye Grant) & Gene York (Michael Woods) are given a baby girl by Sister Yvonne (Megan Leitch) who they have adopted, they name her Delia. At first things go well but as the years pass & Delia (Asia Vieria) grows up Karen becomes suspicious of her as death & disaster follows her, Karen is convinced that she is evil itself. Karen then finds out that she is pregnant but discovers a sinister plot to use her as a surrogate mother for th next Antichrist & gets a shock when she finds out who Delia's real father was...<br /><br />Originally to be directed by Dominique Othenin-Girard who either quit or was sacked & was replaced by Jorge Montesi who completed the film although why he bothered is anyone's guess as Omen IV: The Awakening is absolutely terrible & a disgrace when compared to it illustrious predecessors. The script by Brian Taggert is hilariously bad, I'm not sure whether this nonsense actually looked good as the written word on a piece of paper but there are so many things wrong with it that I find even that hard to believe. As a serious film Omen IV: The AWakening falls flat on it's face & it really does work better if you look at it as a comedy spoof, I mean the scene towards the end when the Detective comes face-to-face with a bunch of zombie carol singers who are singing an ominous Gothic song has to be seen to be believed & I thought it was absolutely hilarious & ridiculous in equal measure. Then there's the pointless difference between this & the other Omen films in that this time it's a young girl, the question I ask here is why? Seriously, why? There's no reason at all & isn't used to any effect at all anyway. Then of course there's the stupid twist at the end which claims Delia has been keeping her brother's embryo inside herself & that in a sinister conspiracy involving a group of Satan worshippers it has been implanted in Karen so she can give birth to the Antichrist is moronic & comes across as just plain daft. At first it has a certain entertainment value in how bad it is but the unintentional hilarity gives way to complete boredom sooner rather than later.<br /><br />It's obviously impossible to know how much of Omen IV: The Awakening was directed by Girard & Montesi but you can sort of tell all was not well behind the camera as it's a shabby, cheap looking poorly made film which was actually made-for-TV & it shows with the bland, flat & unimaginative cinematography & production design. Then there's the total lack of scares, atmosphere, tension & gore which are the main elements that made the previous Omen films so effective.<br /><br />The budget must have been pretty low & the film looks like it was. The best most stylish thing about Omen IV: The Awakening is the final shot in which the camera rises up in the air as Delia walks away into the distance to reveal a crucifix shaped cross made by two overlapping path's but this is the very last shot before the end credits roll which says just about everything. I have to mention the music which sounds awful, more suited to a comedy & is very inappropriate sounding. The acting is alright at best but as usual the kid annoys.<br /><br />Omen IV: The Awakening is rubbish, it's a totally ridiculous film that tries to be serious & just ends up coming across as stupid. The change of director's probably didn't help either, that's still not a excuse though. The last Omen film to date following the original The Omen (1976), Damien: Omen II (1978) & The Final Conflict (1981) all of which are far superior to this.\r\n"
     ]
    }
   ],
   "source": [
    "!head -n 1 imdb_train.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17f8d696",
   "metadata": {},
   "source": [
    "## First training with FastText model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4ac30e68",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Read 5M words\n",
      "Number of words:  281132\n",
      "Number of labels: 2\n",
      "Progress: 100.0% words/sec/thread: 1826810 lr:  0.000000 avg.loss:  0.425605 ETA:   0h 0m 0s\n"
     ]
    }
   ],
   "source": [
    "fast_model = fast.train_supervised('imdb_train.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "131052e8",
   "metadata": {},
   "source": [
    "Let's see the train vocabulary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5ed80c14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the vocabulary size is: 281132\n",
      "\n",
      "This is a slice of it:\n",
      "['the', 'a', 'and', 'of', 'to', 'is', 'in', 'I', 'that', 'this', 'it', '/><br', 'was', 'as', 'with', 'for', 'but', 'The', 'on', 'movie']\n"
     ]
    }
   ],
   "source": [
    "print(f\"the vocabulary size is: {len(fast_model.words)}\\n\\nThis is a slice of it:\\n{fast_model.words[:20]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a6fe6b2",
   "metadata": {},
   "source": [
    "### Results of the model:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50f10eb9",
   "metadata": {},
   "source": [
    "We respectfully copy and paste this print function from FastText documentation to see results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b1373799",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_results(N : int, p : float, r : float) -> None:\n",
    "    print(\"N\\t\" + str(N))\n",
    "    print(\"P@{}\\t{:.3f}\".format(1, p))\n",
    "    print(\"R@{}\\t{:.3f}\".format(1, r))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99f8ded3",
   "metadata": {},
   "source": [
    "So let's compute precision at 1 (P@1) and the recall on the test dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e202d23e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N\t25000\n",
      "P@1\t0.860\n",
      "R@1\t0.860\n"
     ]
    }
   ],
   "source": [
    "print_results(*fast_model.test('imdb_test.txt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "892b6996",
   "metadata": {},
   "source": [
    "And we can compute these metrics for all labels separately:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d975f76a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_labels_results(l_scores : Dict[str, Dict[str, float]]) -> None:\n",
    "    for label in l_scores:\n",
    "        print(f\"label '{label}':\\n\")\n",
    "        print(f\"\\tprecision: {np.round(l_scores[label]['precision'], 3)}\")\n",
    "        print(f\"\\trecall: {np.round(l_scores[label]['recall'], 3)}\")\n",
    "        print(f\"\\tF1 score: {np.round(l_scores[label]['f1score'], 3)}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d6d7b27d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label '__label__1':\n",
      "\n",
      "\tprecision: 0.858\n",
      "\trecall: nan\n",
      "\tF1 score: 1.717\n",
      "\n",
      "label '__label__0':\n",
      "\n",
      "\tprecision: 0.861\n",
      "\trecall: nan\n",
      "\tF1 score: 1.722\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_labels_results(fast_model.test_label('imdb_test.txt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ae963c3",
   "metadata": {},
   "source": [
    "## Pre-processing on IMDB dataset:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c41984f3",
   "metadata": {},
   "source": [
    "### Clean the text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e28a7db7",
   "metadata": {},
   "source": [
    "The text-format is not perfect, we have for exemple '\\t' or '<br\\>' that are formated text. So we will replace all special char by space. And we will also add space before and after '!' to make it a separated word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cfc1d9e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_the_text(text_array : str) -> str:\n",
    "    '''\n",
    "        This function return a list of all word and char in the text in parameters.\n",
    "\n",
    "            Parameters:\n",
    "                    text_array (str): The text in a string format.\n",
    "\n",
    "            Returns:\n",
    "                    result (str) : A list with all the word and char in the inpt text.\n",
    "    '''\n",
    "    \n",
    "    specialChars = \"()\\\\\\''.,;:\\\"?-\" \n",
    "    for specialChar in specialChars:\n",
    "        text_array = text_array.replace(specialChar, ' ')\n",
    "        \n",
    "    text_array = text_array.replace(\"/>\", ' ')\n",
    "    text_array = text_array.replace(\"<br\", ' ')\n",
    "    ''' We add space before and after '!' for the split function '''\n",
    "    text_array = text_array.replace(\"!\", \" ! \")\n",
    "    \n",
    "    return text_array.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b323835",
   "metadata": {},
   "source": [
    "Now we can try the same model but with the clean text and see if this modification change the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "98c92795",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.72 s, sys: 200 ms, total: 3.92 s\n",
      "Wall time: 3.91 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "if not os.path.exists(\"imdb_clean_train.txt\"):\n",
    "    with open(\"imdb_clean_train.txt\", \"wb\") as f:\n",
    "        for i in rand_idx:\n",
    "            entry = imdb['train'][int(i)]\n",
    "            s = f\"__label__{entry['label']} {clean_the_text(entry['text'])}\\n\".encode(\"utf-8\")\n",
    "            f.write(s)\n",
    "    \n",
    "        f.close()\n",
    "        \n",
    "# We shuffle rand_idx to apply with test dataset ...\n",
    "np.random.shuffle(rand_idx)\n",
    "        \n",
    "if not os.path.exists(\"imdb_clean_test.txt\"):\n",
    "    with open(\"imdb_clean_test.txt\", \"wb\") as f:\n",
    "        for i in rand_idx:\n",
    "            entry = imdb['test'][int(i)]\n",
    "            s = f\"__label__{entry['label']} {clean_the_text(entry['text'])}\\n\".encode(\"utf-8\")\n",
    "            f.write(s)\n",
    "    \n",
    "        f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "210d0157",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__label__0 what a time we live in when someone like this joe swan whatever the hell is considered a good filmmaker   or even a filmmaker at all !  where are the new crop of filmmakers with brains and talent    we need them bad  and to hell with mumblecore !       this movie is about nothing  just as the characters in the film stand for nothing  it s this horrible  so called gen y  that is full of bored idiots  some of which declare themselves filmmakers with out bothering to learn anything about the craft before shooting  well  orson welles was a filmmaker  john huston was a filmmaker  fellini was a filmmaker  dreyer was a filmmaker  etc  current films like these show just how stupid young  so called  filmmakers  can be when they believe going out with no script  no direction  no thought  no legit  camerawork   everything shot horribly on dv   no craft of editing  no nothing  stands for  rebellious  or  advanced  film making  nope  it s called ignorance and laziness or just pure masturbation of cinema  and there actually is an in your face  jack off shot   so be ready         look at the early films of any accomplished  indie  filmmaker  linklatter  morris  allen  lynch  hartley  jarmusch  jost  lee  or herzog   none made anything as tedious and aimless as this  yet swan whatever the hell  is still going to sxsw every year and hailed as some kind of gutsy  new talent  it s crap !  i can t imagine anyone liking this  and everything else this so called filmmaker has done  all seen by me  is just as bad  the newer stuff clearly made to appeal to a more mainstream audience  one of the sitcom calling   steer clear  unless you re a friend or family member of those involved   on second thought  if you re a family member or friend you d probably be embarrassed to see a family member or friend in such compromising situations         utter garbage  this isn t art  this is the ultimate opposite of it \r\n"
     ]
    }
   ],
   "source": [
    "!head -n 1 imdb_clean_train.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6d5ff022",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Read 6M words\n",
      "Number of words:  80799\n",
      "Number of labels: 2\n",
      "Progress: 100.0% words/sec/thread: 2116883 lr:  0.000000 avg.loss:  0.390082 ETA:   0h 0m 0s100.0% words/sec/thread: 2117159 lr: -0.000008 avg.loss:  0.390082 ETA:   0h 0m 0s\n"
     ]
    }
   ],
   "source": [
    "fast_model_clean = fast.train_supervised('imdb_clean_train.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f13e1733",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the vocabulary size is: 80799\n",
      "\n",
      "This is a slice of it:\n",
      "['the', 'and', 'a', 'of', 'to', 'is', 'it', 'in', 'i', 'this', 'that', 's', 'was', 'as', 'for', 'with', 'movie', 'but', 'film', 'you']\n"
     ]
    }
   ],
   "source": [
    "print(f\"the vocabulary size is: {len(fast_model_clean.words)}\\n\\nThis is a slice of it:\\n{fast_model_clean.words[:20]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9647f111",
   "metadata": {},
   "source": [
    "#### Result of clean model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fe13767c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N\t25000\n",
      "P@1\t0.879\n",
      "R@1\t0.879\n"
     ]
    }
   ],
   "source": [
    "print_results(*fast_model_clean.test('imdb_clean_test.txt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e60ed600",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label '__label__0':\n",
      "\n",
      "\tprecision: 0.882\n",
      "\trecall: nan\n",
      "\tF1 score: 1.765\n",
      "\n",
      "label '__label__1':\n",
      "\n",
      "\tprecision: 0.875\n",
      "\trecall: nan\n",
      "\tF1 score: 1.75\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_labels_results(fast_model_clean.test_label('imdb_clean_test.txt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3549de4",
   "metadata": {},
   "source": [
    "So we can see that in average we have a upgrade of our result of 0.02. It's not a huge upgrade but it's still ok for a few more seconds of calculation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60794076",
   "metadata": {},
   "source": [
    "### Stemming the data:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7b643a8",
   "metadata": {},
   "source": [
    "First of all we need to create a function that stemme a word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "77e4fac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "re_word = re.compile(r\"^\\w+$\")\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "\n",
    "stem_word: Callable[[str], str] = lambda w : stemmer.stem(w.lower()) if re_word.match(w) else w"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "545dd43c",
   "metadata": {},
   "source": [
    "Now we have ti create a function that apply stemming to a whole text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4f981507",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stemming_text(text : str) -> str:\n",
    "    '''\n",
    "        This function steeming the text in parameter and return in\n",
    "        \n",
    "        Parameters :\n",
    "                text (str) : the text to stemming\n",
    "                \n",
    "        Returns :\n",
    "                return_text (str) : the text stemmed\n",
    "    '''\n",
    "    list_of_words = text.split(\" \")\n",
    "    \n",
    "    list_of_words = [stem_word(word) for word in list_of_words]\n",
    "    \n",
    "    return_text = \" \".join(list_of_words)\n",
    "    \n",
    "    return return_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08f2d1c6",
   "metadata": {},
   "source": [
    "Now with this function we can create the new model where we use the stemming for all text before write them in the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a3da7b59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.31 ms, sys: 60 µs, total: 1.37 ms\n",
      "Wall time: 1.17 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "if not os.path.exists(\"imdb_stemmed_train.txt\"):\n",
    "    with open(\"imdb_stemmed_train.txt\", \"wb\") as f:\n",
    "        for i in rand_idx:\n",
    "            entry = imdb['train'][int(i)]\n",
    "            s = f\"__label__{entry['label']} {stemming_text(entry['text'])}\\n\".encode(\"utf-8\")\n",
    "            f.write(s)\n",
    "    \n",
    "        f.close()\n",
    "        \n",
    "# We shuffle rand_idx to apply with test dataset ...\n",
    "np.random.shuffle(rand_idx)\n",
    "        \n",
    "if not os.path.exists(\"imdb_stemmed_test.txt\"):\n",
    "    with open(\"imdb_stemmed_test.txt\", \"wb\") as f:\n",
    "        for i in rand_idx:\n",
    "            entry = imdb['test'][int(i)]\n",
    "            s = f\"__label__{entry['label']} {stemming_text(entry['text'])}\\n\".encode(\"utf-8\")\n",
    "            f.write(s)\n",
    "    \n",
    "        f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b2a41939",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__label__1 i watch this last night after not have seen it for sever years. it realli is a fun littl film, with a bunch of face you didn't know were in it. arkin shine as always. check it out; you won't be dissappointed. by the way, it was just releas on dvd and contrari to it packaging, it is widescreen. the transfer is rather poor, but at least the whole movi is visible. ;-)\r\n"
     ]
    }
   ],
   "source": [
    "!head -n 1 imdb_stemmed_train.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e596fd3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Read 5M words\n",
      "Number of words:  245430\n",
      "Number of labels: 2\n",
      "Progress: 100.0% words/sec/thread: 2100794 lr:  0.000000 avg.loss:  0.418999 ETA:   0h 0m 0s\n"
     ]
    }
   ],
   "source": [
    "fast_model_stemmed = fast.train_supervised('imdb_stemmed_train.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e79a8d61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the vocabulary size is: 245430\n",
      "\n",
      "This is a slice of it:\n",
      "['the', 'a', 'and', 'of', 'to', 'is', 'in', 'it', 'i', 'this', 'that', '/><br', 'was', 'as', 'for', 'with', 'but', 'movi', 'film', 'be']\n"
     ]
    }
   ],
   "source": [
    "print(f\"the vocabulary size is: {len(fast_model_stemmed.words)}\\n\\nThis is a slice of it:\\n{fast_model_stemmed.words[:20]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48116788",
   "metadata": {},
   "source": [
    "#### Result of stemmed model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "bbdcb864",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N\t25000\n",
      "P@1\t0.861\n",
      "R@1\t0.861\n"
     ]
    }
   ],
   "source": [
    "print_results(*fast_model_stemmed.test('imdb_stemmed_test.txt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8a8cd6e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label '__label__1':\n",
      "\n",
      "\tprecision: 0.857\n",
      "\trecall: nan\n",
      "\tF1 score: 1.715\n",
      "\n",
      "label '__label__0':\n",
      "\n",
      "\tprecision: 0.864\n",
      "\trecall: nan\n",
      "\tF1 score: 1.728\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_labels_results(fast_model_stemmed.test_label('imdb_stemmed_test.txt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d283b8c0",
   "metadata": {},
   "source": [
    "The stemmed model don't change the result or juste of 0.002 in the label 1 so the result is not convincing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27ee55bb",
   "metadata": {},
   "source": [
    "### Lemming the data:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67dfb962",
   "metadata": {},
   "source": [
    "Firstly, we need to download the english model of Spacy lemmatization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "50012030",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m spacy download en_core_web_sm > output_dl.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5bc8baf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading the small English model\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "152368f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 11min 19s, sys: 650 ms, total: 11min 20s\n",
      "Wall time: 11min 20s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Shuffle order again\n",
    "np.random.shuffle(rand_idx)\n",
    "\n",
    "if not os.path.exists(\"lemmed_imdb_train.txt\"):\n",
    "    with open(\"lemmed_imdb_train.txt\", \"wb\") as f:\n",
    "        for i in rand_idx:\n",
    "            entry = imdb['train'][int(i)]\n",
    "            \n",
    "            # lemmatize before writting\n",
    "            lemmed_text = ' '.join([token.lemma_ for token in nlp(entry['text'])])\n",
    "            s = f\"__label__{entry['label']} {lemmed_text}\\n\".encode(\"utf-8\")\n",
    "            f.write(s)\n",
    "    \n",
    "        f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb82b8a6",
   "metadata": {},
   "source": [
    "Do it in test dataset also:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "22937706",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle order again\n",
    "np.random.shuffle(rand_idx)\n",
    "\n",
    "if not os.path.exists(\"lemmed_imdb_test.txt\"):\n",
    "    with open(\"lemmed_imdb_test.txt\", \"wb\") as f:\n",
    "        for i in rand_idx:\n",
    "            entry = imdb['test'][int(i)]\n",
    "            \n",
    "            # lemmatize before writting\n",
    "            lemmed_text = ' '.join([token.lemma_ for token in nlp(entry['text'])])\n",
    "            s = f\"__label__{entry['label']} {lemmed_text}\\n\".encode(\"utf-8\")\n",
    "            f.write(s)\n",
    "    \n",
    "        f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8acdb4b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Read 6M words\n",
      "Number of words:  106199\n",
      "Number of labels: 2\n",
      "Progress: 100.0% words/sec/thread: 2445864 lr:  0.000000 avg.loss:  0.416560 ETA:   0h 0m 0s\n"
     ]
    }
   ],
   "source": [
    "fast_model_lemming = fast.train_supervised('lemmed_imdb_train.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6d87da66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the vocabulary size is: 106199\n",
      "\n",
      "This is a slice of it:\n",
      "['the', 'be', ',', '.', 'and', 'a', 'of', 'to', 'it', 'I', 'in', 'this', 'that', '\"', 'have', '-', '/><br', 'movie', 'film', 'as']\n"
     ]
    }
   ],
   "source": [
    "print(f\"the vocabulary size is: {len(fast_model_lemming.words)}\\n\\nThis is a slice of it:\\n{fast_model_lemming.words[:20]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8793d9e0",
   "metadata": {},
   "source": [
    "#### Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e5875f31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N\t25000\n",
      "P@1\t0.867\n",
      "R@1\t0.867\n"
     ]
    }
   ],
   "source": [
    "print_results(*fast_model_lemming.test('lemmed_imdb_test.txt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "5881726e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label '__label__0':\n",
      "\n",
      "\tprecision: 0.872\n",
      "\trecall: nan\n",
      "\tF1 score: 1.743\n",
      "\n",
      "label '__label__1':\n",
      "\n",
      "\tprecision: 0.862\n",
      "\trecall: nan\n",
      "\tF1 score: 1.723\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_labels_results(fast_model_lemming.test_label('lemmed_imdb_test.txt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4368f4ed",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7404829f",
   "metadata": {},
   "source": [
    "## Hyperparameters tunning:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf2a357e",
   "metadata": {},
   "source": [
    "We need to extract a validation set of our train dataset to avoid a tunning validation on test dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "02366b7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-r--r-- 1 leherlemaxime leherlemaxime 26719815 Oct  4 19:16 ./tuning_aa\r\n",
      "-rw-r--r-- 1 leherlemaxime leherlemaxime  6713008 Oct  4 19:16 ./tuning_ab\r\n"
     ]
    }
   ],
   "source": [
    "# split command will copy and separate file into set of files of 20000 lines.\n",
    "# Train file have 25.000 lines, so train will have 20.000 lines and validation 5.000 lines.\n",
    "!split -l20000 \"imdb_train.txt\" tuning_\n",
    "\n",
    "!ls -l ./tuning_*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e26d837",
   "metadata": {},
   "source": [
    "### Try the default hyperparameter tunning of FastText:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "3f315ad8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Progress: 100.0% Trials:    9 Best score:  0.884740 ETA:   0h 0m 0s\n",
      "Training again with best arguments\n",
      "Read 4M words\n",
      "Number of words:  244423\n",
      "Number of labels: 2\n",
      "Progress: 100.0% words/sec/thread:  767266 lr:  0.000000 avg.loss:  0.048147 ETA:   0h 0m 0s\n"
     ]
    }
   ],
   "source": [
    "tunning_fast_model = fast.train_supervised(input='tuning_aa', autotuneValidationFile='tuning_ab', autotuneMetric=\"f1:__label__0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72fcbfef",
   "metadata": {},
   "source": [
    "Let's compute global metrics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b0845727",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N\t25000\n",
      "P@1\t0.883\n",
      "R@1\t0.883\n"
     ]
    }
   ],
   "source": [
    "print_results(*tunning_fast_model.test('imdb_test.txt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69d8a41a",
   "metadata": {},
   "source": [
    "It looks to give better results with default hyperparameter tunning. But how labels scores change ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "65768679",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label '__label__1':\n",
      "\n",
      "\tprecision: 0.882\n",
      "\trecall: nan\n",
      "\tF1 score: 1.764\n",
      "\n",
      "label '__label__0':\n",
      "\n",
      "\tprecision: 0.884\n",
      "\trecall: nan\n",
      "\tF1 score: 1.768\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_labels_results(tunning_fast_model.test_label('imdb_test.txt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8508810",
   "metadata": {},
   "source": [
    "Results are better with tunning and we highlight that optimize f1 result on __negative__ label induces better improvements on __positive__ label. The reason is because we juste have two labels and __negative__ label had less wrongly classification."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87ac6021",
   "metadata": {},
   "source": [
    "## Conclusion:"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
