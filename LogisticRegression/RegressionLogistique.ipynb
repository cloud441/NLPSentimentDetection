{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "46cda421",
   "metadata": {},
   "source": [
    "# Logistique Regression model  (TP nÂ°1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22c4981c",
   "metadata": {},
   "source": [
    "## Import"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "058e2761",
   "metadata": {},
   "source": [
    "First of all, we will need several python packages in our method :\n",
    "\n",
    "- dataset : we need it to download the dataset\n",
    "- math : we need it for the log function\n",
    "- sklearn : we will use the __LogisticRegression__ function to create our model and __precision_recall_fscore_support__ to calculate the score of our models\n",
    "- numpy : to define the random seed to make the results reproducible\n",
    "- panda : to manipulate the dataset\n",
    "- typing : to type our functions\n",
    "- nltk and spacy : for lemming and stemming steps\n",
    "- re : for regex manipulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5104d738",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "import math\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import List\n",
    "from typing import Callable\n",
    "from nltk import tokenize\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "import spacy\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d804783b",
   "metadata": {},
   "source": [
    "## The dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d98e609b",
   "metadata": {},
   "source": [
    "We download the dataset :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "25c2cc67",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset imdb (/home/leherlemaxime/.cache/huggingface/datasets/imdb/plain_text/1.0.0/e3c66f1788a67a89c7058d97ff62b6c30531e05b549de56d3ab91891f0561f9a)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7283d133a6df467f83ae5cec64068c88",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = datasets.load_dataset('imdb')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e10f4b5",
   "metadata": {},
   "source": [
    "Now take a look at the dataset format :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2e541258",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 25000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 25000\n",
       "    })\n",
       "    unsupervised: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 50000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "052a1b9d",
   "metadata": {},
   "source": [
    "So we can see that our dataset is composed of 2 parts, a __train__ part and a __test__ part which both contain 25000 elements. Each element of our dataset has 2 components, the first is the text and the second is the corresponding label. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc158850",
   "metadata": {},
   "source": [
    "And we can also see what an element of our dataset looks like :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "02885564",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': 'Titanic has to be one of my all-time favorite movies. It has its problems (what movies don\\'t) but still, it\\'s enjoyable.<br /><br />When I stumble across someone who asks me why I like Titanic, I suppose my first reaction is \"wait a minute, you don\\'t?\" I know so many people who don\\'t like this movie, and I\\'m not saying I don\\'t see why. \"The love story is too cheesy\" well, yes but isn\\'t it enjoyable and moving? All right, the love story between Jack and Rose is very unrealistic, everyone knows that love like this doesn\\'t actually exist. But this is a movie, doesn\\'t everyone enjoy watching a beautiful story that lets us slip slightly into fantasy for a while? The next complaint, DiCaprio and Winslet are terrible actors. Well, OK, in this movie, I agree that they do not perform to their full potentials. However I think it\\'s unfair to say that they are terrible actors. I personally think they are both very talented actors who unfortunately are very famous for a movie that they are not amazing in. But the roles they are given are simple, and the characters seem real enough that you can care about them quite a bit, but I agree with many people that they did not do as well as could have been expected.<br /><br />And finally, if one is going to complain that they don\\'t like this movie because they hate romance, or because they hate history, or tragic movies, then I\\'m sorry but why on earth did they go and see a movie that is so clearly all of these things. It\\'s like people who complain The Dark Knight is a bad movie because they hate action movies. Simply for being a movie, not because you dislike the genre, this IS a good movie.<br /><br />Well deserving of its Oscars, in particular, Best Cinematography, which I find to be the best I\\'ve ever seen in a movie save maybe the Lord of the Rings trilogy.<br /><br />I know some of the writing fails, such as the constant screaming of each other\\'s names throughout the movie. The flashback portion of the story can be quite weak at times, but overall it\\'s an amazing achievement in making the Titanic look so real, and the sinking feel so epic.<br /><br />I understand why a lot of people dislike this movie, but for the most part it boils down to them disliking the fundamental idea, such as it being a love story, rather than them thinking the movie in and of itself is poorly constructed.<br /><br />I can tell you that I have read more than five books about the Titanic, including memoirs form the day it happened, and this movie is extremely historically accurate save just a few faults. The only main ones I can find is that the piping should be threaded copper, not steel, and the iceberg looks fairly unrealistic as is the scene where they hit it.<br /><br />I give this movie 10/10, not because I like romance movies, but simply because it\\'s an outstanding cinematic achievement, that leaves one feeling horrified by the realistic adaptation of events.',\n",
       " 'label': 1}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[\"train\"][42]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b9dad34",
   "metadata": {},
   "source": [
    "We also need to define the types that we will use in the typing of our functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "86e15c4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_type = List[int]\n",
    "DataFrame = pd.core.frame.DataFrame\n",
    "list_of_words = List[str]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64436d69",
   "metadata": {},
   "source": [
    "## Random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d3c44e6",
   "metadata": {},
   "source": [
    "We set the random seed to to ensure that our results are reproducible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "349b36b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_seed = 38\n",
    "random = np.random.default_rng(random_seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3026cf1",
   "metadata": {},
   "source": [
    "## First model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "851e7307",
   "metadata": {},
   "source": [
    "### Creation of the features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7128ddba",
   "metadata": {},
   "source": [
    "For this model we will need to create features from our text to use as input to our logistic regression. We will use the following features:\n",
    "\n",
    "- 1 if \"no\" appear in the doc, 0 otherwise\n",
    "- The count of first and second pronouns in the document\n",
    "- The count of \".\" => number of classic sentences\n",
    "- 1 if \"!\" is in the document, 0 otherwise\n",
    "- log(word count in the document)\n",
    "- Number of words in the document which are in the positive lexicon\n",
    "- Number of words in the document which are in the negative lexicon\n",
    "- Number of words in the document wich are in the lexicon but netral\n",
    "\n",
    "So we have used the basic features proposed then we have added 2 new features and we are now going to explain our choice.\n",
    "For the feature __sentences count__ we noticed that often the positive criticism contained shorter sentences, indeed they are less detailed than the sentences with negative conotation so that's why we wanted to add this feature.  And then for __netral words__ we decided to choose this because we already use the number of words but here we use the number of interesting words that add meaning to the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2e837020",
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_array_to_feature(word_array : list_of_words) -> feature_type:\n",
    "    feature = []\n",
    "    \n",
    "    ''' No feature '''\n",
    "    \n",
    "    if (\"no\" in word_array):\n",
    "        feature.append(1)\n",
    "    else:\n",
    "        feature.append(0)\n",
    "        \n",
    "    ''' Pronouns feature '''\n",
    "    \n",
    "    valid_pronouns = [\"i\", \"me\", \"my\", \"mine\", \"myself\", \"you\", \"your\", \"yours\", \"yourself\", \"we\", \"us\", \"our\", \"ourselves\"]\n",
    "    pronouns_count = 0\n",
    "    for elem in word_array:\n",
    "        if (elem in valid_pronouns):\n",
    "            pronouns_count += 1\n",
    "            \n",
    "    feature.append(pronouns_count)\n",
    "    \n",
    "    ''' \".\" feature '''\n",
    "    \n",
    "    feature.append(word_array.count(\".\"))\n",
    "            \n",
    "    ''' ! feature '''\n",
    "        \n",
    "    if (\"!\" in word_array):\n",
    "        feature.append(1)\n",
    "    else:\n",
    "        feature.append(0)\n",
    "        \n",
    "    ''' log(nb_word) feature '''\n",
    "    \n",
    "    feature.append(math.log(len(word_array)))\n",
    "    \n",
    "    ''' positive, negative and neutral feature '''\n",
    "    positive_count = 0\n",
    "    negative_count = 0\n",
    "    netral_count = 0\n",
    "    \n",
    "    for elem in word_array :\n",
    "        if ((elem in dict_value) and (float(dict_value[elem]) >= 1.5)):\n",
    "            positive_count += 1\n",
    "        elif ((elem in dict_value) and (float(dict_value[elem]) <= -1.5)):\n",
    "            negative_count += 1\n",
    "        elif ((elem in dict_value)):\n",
    "            netral_count += 1\n",
    "            \n",
    "    feature.append(positive_count)\n",
    "    feature.append(negative_count)\n",
    "    feature.append(netral_count)\n",
    "    \n",
    "    return feature"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74e6b4f4",
   "metadata": {},
   "source": [
    "### Create the lexicon"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98d946e3",
   "metadata": {},
   "source": [
    "We have speak about a lexcion so now we need to create it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "23c3e6ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Open and read all line of the file '''\n",
    "file = open(\"vader_lexicon.txt\", \"r\", encoding=\"utf-8\")\n",
    "lines = file.readlines()\n",
    "file.close()\n",
    "\n",
    "''' We define a list of all our line but in the good format '''\n",
    "good_format = []\n",
    "\n",
    "''' For each line we replace all tablutaion by space and we split to get the 2 first elements '''\n",
    "for line in lines:\n",
    "    line_temp = line.replace(\"\\t\", \" \")\n",
    "    line_temp = line_temp.split(\" \")\n",
    "    line_temp = line_temp[:2]\n",
    "    good_format.append(line_temp)\n",
    "\n",
    "''' Now we create a dict from our list to use it after '''\n",
    "dict_value = dict(good_format)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81a928a6",
   "metadata": {},
   "source": [
    "### Formatting the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "616e9354",
   "metadata": {},
   "source": [
    "We need a function that can take our dataset and make it usable for our other function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "044f3bab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataset_to_array(dataset : DataFrame) -> (List[str], List[int], List[str], List[int]):\n",
    "    '''\n",
    "        This function take the dataset and create and array from this dataset\n",
    "        \n",
    "        Parameters :\n",
    "                dataset (dataset): it's the input dataset thta we want to format in array\n",
    "                \n",
    "        Returns:\n",
    "                x_train (list[str) : We have all the list avec all word of text in the train part of the datatset\n",
    "                y_train (list[int]) : We have a list of all label of text in the train part  of the dataset\n",
    "                x_test (list[str]) : We have all the list avec all word of text in the test part of the datatset\n",
    "                y_test (list[int]) : We have a list of all label of text in the test part  of the dataset\n",
    "    '''\n",
    "    x_train = []\n",
    "    y_train = []\n",
    "    x_test = []\n",
    "    y_test = []\n",
    "    \n",
    "    for elem in dataset[\"train\"]:\n",
    "        y_train.append(elem[\"label\"])\n",
    "        x_train.append(elem[\"text\"])\n",
    "        \n",
    "    for elem in dataset[\"test\"]:\n",
    "        y_test.append(elem[\"label\"])\n",
    "        x_test.append(elem[\"text\"])\n",
    "        \n",
    "    return x_train, y_train, x_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7b14963c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 18.3 s, sys: 2.69 s, total: 21 s\n",
      "Wall time: 20.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "x_train, y_train, x_test, y_test = dataset_to_array(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65bd6e9a",
   "metadata": {},
   "source": [
    "We can see that formatting the dataset don't take a lot's of time : 2.22 seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6af87c26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 12.1 s, sys: 2.17 ms, total: 12.1 s\n",
      "Wall time: 12.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "x_train_feature = [word_array_to_feature(elem.split(\" \")) for elem in x_train]\n",
    "x_test_feature = [word_array_to_feature(elem.split(\" \")) for elem in x_test]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d553db1",
   "metadata": {},
   "source": [
    "The creation of features just take 3.34 seconds wich is fast. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1969599e",
   "metadata": {},
   "source": [
    "We create a __LogisticRegression__ model from sikitleran and we train it with the train data.\n",
    "We also set the random_state to the variable __random_seed__ to control the random and make the result reproducible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "50a322ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.54 s, sys: 1.41 s, total: 2.95 s\n",
      "Wall time: 411 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "clf = LogisticRegression(random_state=random_seed).fit(x_train_feature, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de2ba4a3",
   "metadata": {},
   "source": [
    "The training time is really fast."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ee544f9",
   "metadata": {},
   "source": [
    "#### Result of the first model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cb35cde2",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = clf.predict(x_test_feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4c48b903",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_formated_result(result):\n",
    "    print(\"For labbel 0 :\")\n",
    "    print(\"\\t - number of entry : \", result[3][0])\n",
    "    print(\"\\t - precision : \", result[0][0])\n",
    "    print(\"\\t - recall : \", result[2][0])\n",
    "    print(\"\\t - fbeta_score : \", result[1][0])\n",
    "    print(\"\\n\\nFor labbel 1 :\")\n",
    "    print(\"\\t - number of entry : \", result[3][1])\n",
    "    print(\"\\t - precision : \", result[0][1])\n",
    "    print(\"\\t - recall : \", result[2][1])\n",
    "    print(\"\\t - fbeta_score : \", result[1][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "752f9751",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For labbel 0 :\n",
      "\t - number of entry :  12500\n",
      "\t - precision :  0.6965906303654648\n",
      "\t - recall :  0.6890137883627835\n",
      "\t - fbeta_score :  0.6816\n",
      "\n",
      "\n",
      "For labbel 1 :\n",
      "\t - number of entry :  12500\n",
      "\t - precision :  0.6883076200172292\n",
      "\t - recall :  0.6956349677470418\n",
      "\t - fbeta_score :  0.70312\n"
     ]
    }
   ],
   "source": [
    "print_formated_result(precision_recall_fscore_support(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "672d4824",
   "metadata": {},
   "source": [
    "First of all we can see that we have 12500 elements of each labels.\n",
    "\n",
    "So we can see that the results are quite correct. We have a precission and a recall between __0,68__ and __0,70__ on each label. The fbeta_score, which is the weighted harmonic mean of precision and recall, is of __0,68__ for label __0__ and __0.70__ for label __1__."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7eec3be",
   "metadata": {},
   "source": [
    "## Pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7796342b",
   "metadata": {},
   "source": [
    "### Clean the text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f59f23a",
   "metadata": {},
   "source": [
    "As we can see in the text of the elements there are still some formatting characters in fact there are still __\"\\t\"__, __\";\"__ and so on.\n",
    "So we will define a list of words and formatting characters that we will remove from the text. Moreover we will also put spaces before and after the __\"!\"__ so that they are considered as words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e920fe23",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_the_text(text_array : str) -> List[str]:\n",
    "    '''\n",
    "        This function return a list of all word and char in the text in parameters.\n",
    "\n",
    "            Parameters:\n",
    "                    text_array (str): The text in a string format.\n",
    "\n",
    "            Returns:\n",
    "                    result_array (list[str]) : A list with all the word and char in the inpt text.\n",
    "    '''\n",
    "    \n",
    "    specialChars = \"()\\\\\\'',;:\\\"?-\" \n",
    "    for specialChar in specialChars:\n",
    "        text_array = text_array.replace(specialChar, ' ')\n",
    "        \n",
    "    text_array = text_array.replace(\".\", \" . \")\n",
    "    text_array = text_array.replace(\"/>\", ' ')\n",
    "    text_array = text_array.replace(\"<br\", ' ')\n",
    "    ''' As say before we add space before and after '!' for the split function '''\n",
    "    text_array = text_array.replace(\"!\", \" ! \")\n",
    "    \n",
    "    \n",
    "    return text_array.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f2664dcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataset_to_array_clean(dataset : DataFrame) -> (List[str], List[int], List[str], List[int]):\n",
    "    '''\n",
    "        This function take the dataset and create and array from this dataset\n",
    "        \n",
    "        Parameters :\n",
    "                dataset (dataset): it's the input dataset thta we want to format in array\n",
    "                \n",
    "        Returns:\n",
    "                x_train (list[str]) : We have all the list avec all word of text in the train part of the datatset\n",
    "                y_train (list[int]) : We have a list of all label of text in the train part  of the dataset\n",
    "                x_test (list[str]) : We have all the list avec all word of text in the test part of the datatset\n",
    "                y_test (list[int]) : We have a list of all label of text in the test part  of the dataset\n",
    "    '''\n",
    "    x_train = []\n",
    "    y_train = []\n",
    "    x_test = []\n",
    "    y_test = []\n",
    "    \n",
    "    for elem in dataset[\"train\"]:\n",
    "        y_train.append(elem[\"label\"])\n",
    "        x_train.append(clean_the_text(elem[\"text\"]))\n",
    "        \n",
    "    for elem in dataset[\"test\"]:\n",
    "        y_test.append(elem[\"label\"])\n",
    "        x_test.append(clean_the_text(elem[\"text\"]))\n",
    "        \n",
    "    return x_train, y_train, x_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3361a040",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 6.52 s, sys: 302 ms, total: 6.82 s\n",
      "Wall time: 6.71 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "x_train_clean, y_train_clean, x_test_clean, y_test_clean = dataset_to_array(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1420143c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 10.9 s, sys: 0 ns, total: 10.9 s\n",
      "Wall time: 10.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "x_train_feature_clean = [word_array_to_feature(elem.split(\" \")) for elem in x_train_clean]\n",
    "x_test_feature_clean = [word_array_to_feature(elem.split(\" \")) for elem in x_test_clean]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5bf5886c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.64 s, sys: 1.13 s, total: 2.76 s\n",
      "Wall time: 383 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "clf_clean = LogisticRegression(random_state=random_seed).fit(x_train_feature_clean, y_train_clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "927ea771",
   "metadata": {},
   "source": [
    "Overall we can see that cleaning the text adds computational time but it remains negligible."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab2f1eee",
   "metadata": {},
   "source": [
    "Now take a look at result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f48de2c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_clean = clf_clean.predict(x_test_feature_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "95ab8ef3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For labbel 0 :\n",
      "\t - number of entry :  12500\n",
      "\t - precision :  0.6965906303654648\n",
      "\t - recall :  0.6890137883627835\n",
      "\t - fbeta_score :  0.6816\n",
      "\n",
      "\n",
      "For labbel 1 :\n",
      "\t - number of entry :  12500\n",
      "\t - precision :  0.6883076200172292\n",
      "\t - recall :  0.6956349677470418\n",
      "\t - fbeta_score :  0.70312\n"
     ]
    }
   ],
   "source": [
    "print_formated_result(precision_recall_fscore_support(y_test_clean, y_pred_clean))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09070682",
   "metadata": {},
   "source": [
    "We can see with the scores that the results do not change the results, in fact it only modifies the features a little and it is not enough to be able to modify the prediction of an element, so we will not keep this optimization for the rest of our notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ffb1698",
   "metadata": {},
   "source": [
    "### Stemming"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60fe271d",
   "metadata": {},
   "source": [
    "We will now try to use stemming on our text to see if it changes our results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4b33c106",
   "metadata": {},
   "outputs": [],
   "source": [
    "re_word = re.compile(r\"^\\w+$\")\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "\n",
    "stem_word: Callable[[str], str] = lambda w : stemmer.stem(w.lower()) if re_word.match(w) else w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "839012c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stemmed_text(text : str) -> str:\n",
    "    '''\n",
    "        This function steeming the text in parameter and return in\n",
    "        \n",
    "        Parameters :\n",
    "                text (str) : the text to stemming\n",
    "                \n",
    "        Returns :\n",
    "                return_text (str) : the text stemmed\n",
    "    '''\n",
    "    list_of_words = text.split(\" \")\n",
    "    \n",
    "    list_of_words = [stem_word(word) for word in list_of_words]\n",
    "    \n",
    "    return_text = \" \".join(list_of_words)\n",
    "    \n",
    "    return return_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31e018cd",
   "metadata": {},
   "source": [
    "Now define an other function for this optimisation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7ce37752",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataset_to_array_stemmed(dataset : DataFrame) -> (List[list_of_words], List[int], List[list_of_words], List[int]):\n",
    "    '''\n",
    "        This function take the dataset and create and array from this dataset\n",
    "        \n",
    "        Parameters :\n",
    "                dataset (dataset): it's the input dataset thta we want to format in array\n",
    "                \n",
    "        Returns:\n",
    "                x_train (list[list[str]]) : We have all the list avec all word of text in the train part of the datatset\n",
    "                y_train (list[int]) : We have a list of all label of text in the train part  of the dataset\n",
    "                x_test (list[list[str]]) : We have all the list avec all word of text in the test part of the datatset\n",
    "                y_test (list[int]) : We have a list of all label of text in the test part  of the dataset\n",
    "    '''\n",
    "    x_train = []\n",
    "    y_train = []\n",
    "    x_test = []\n",
    "    y_test = []\n",
    "    \n",
    "    for elem in dataset[\"train\"]:\n",
    "        y_train.append(elem[\"label\"])\n",
    "        x_train.append(stemmed_text(elem[\"text\"]))\n",
    "        \n",
    "    for elem in dataset[\"test\"]:\n",
    "        y_test.append(elem[\"label\"])\n",
    "        x_test.append(stemmed_text(elem[\"text\"]))\n",
    "        \n",
    "    return x_train, y_train, x_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "af4bd6c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2min 37s, sys: 224 ms, total: 2min 37s\n",
      "Wall time: 2min 37s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "x_train_stemmed, y_train_stemmed, x_test_stemmed, y_test_stemmed = dataset_to_array_stemmed(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "05a0ad85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 9 s, sys: 0 ns, total: 9 s\n",
      "Wall time: 8.99 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "x_train_feature_stemmed = [word_array_to_feature(elem.split(\" \")) for elem in x_train_stemmed]\n",
    "x_test_feature_stemmed = [word_array_to_feature(elem.split(\" \")) for elem in x_test_stemmed]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "bde63c3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 643 ms, sys: 551 ms, total: 1.19 s\n",
      "Wall time: 184 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "clf_stemmed = LogisticRegression(random_state=random_seed).fit(x_train_feature_stemmed, y_train_stemmed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9de1fc01",
   "metadata": {},
   "source": [
    "We can now test out new optimisation and see if it's change the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3a0d8428",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_stemmed = clf_stemmed.predict(x_test_feature_stemmed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "557920ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For labbel 0 :\n",
      "\t - number of entry :  12500\n",
      "\t - precision :  0.6702224427354668\n",
      "\t - recall :  0.6591306469320538\n",
      "\t - fbeta_score :  0.6484\n",
      "\n",
      "\n",
      "For labbel 1 :\n",
      "\t - number of entry :  12500\n",
      "\t - precision :  0.6594871000232432\n",
      "\t - recall :  0.6700515605935372\n",
      "\t - fbeta_score :  0.68096\n"
     ]
    }
   ],
   "source": [
    "print_formated_result(precision_recall_fscore_support(y_test_stemmed, y_pred_stemmed))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5f247c1",
   "metadata": {},
   "source": [
    "So we can see that once again the computation time is increased but in a rather negligible way because we remain in very short computation times. On the other hand our results are less good indeed that comes from the fact that the stemming changes the words but can thus not make them match with the dictionary we will thus have to create a new dictionary to match with the stemming."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "46fe35cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Open and read all line of the file '''\n",
    "file = open(\"vader_lexicon.txt\", \"r\", encoding=\"utf-8\")\n",
    "lines = file.readlines()\n",
    "file.close()\n",
    "\n",
    "''' We define a list of all our line but in the good format '''\n",
    "good_format_stemming = []\n",
    "\n",
    "''' For each line we replace all tablutaion by space and we split to get the 2 first elements '''\n",
    "for line in lines:\n",
    "    line_temp = line.replace(\"\\t\", \" \")\n",
    "    line_temp = line_temp.split(\" \")\n",
    "    line_temp = line_temp[:2]\n",
    "    line_temp[0] = stem_word(line_temp[0])\n",
    "    good_format_stemming.append(line_temp)\n",
    "\n",
    "''' Now we create a dict from our list to use it after '''\n",
    "dict_value_stemming = dict(good_format_stemming)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1bfdd5b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_array_to_feature_stemming(word_array : list_of_words) -> feature_type:\n",
    "    feature = []\n",
    "    \n",
    "    ''' No feature '''\n",
    "    \n",
    "    if (\"no\" in word_array):\n",
    "        feature.append(1)\n",
    "    else:\n",
    "        feature.append(0)\n",
    "        \n",
    "    ''' Pronouns feature '''\n",
    "    \n",
    "    valid_pronouns = [\"i\", \"me\", \"my\", \"mine\", \"myself\", \"you\", \"your\", \"yours\", \"yourself\", \"we\", \"us\", \"our\", \"ourselves\"]\n",
    "    pronouns_count = 0\n",
    "    for elem in word_array:\n",
    "        if (elem in valid_pronouns):\n",
    "            pronouns_count += 1\n",
    "            \n",
    "    feature.append(pronouns_count)\n",
    "    \n",
    "    ''' \".\" feature '''\n",
    "    \n",
    "    feature.append(word_array.count(\".\"))\n",
    "            \n",
    "    ''' ! feature '''\n",
    "        \n",
    "    if (\"!\" in word_array):\n",
    "        feature.append(1)\n",
    "    else:\n",
    "        feature.append(0)\n",
    "        \n",
    "    ''' log(nb_word) feature '''\n",
    "    \n",
    "    feature.append(math.log(len(word_array)))\n",
    "    \n",
    "    ''' positive, negative and neutral feature '''\n",
    "    positive_count = 0\n",
    "    negative_count = 0\n",
    "    netral_count = 0\n",
    "    \n",
    "    for elem in word_array :\n",
    "        if ((elem in dict_value_stemming) and (float(dict_value_stemming[elem]) >= 1.5)):\n",
    "            positive_count += 1\n",
    "        elif ((elem in dict_value_stemming) and (float(dict_value_stemming[elem]) <= -1.5)):\n",
    "            negative_count += 1\n",
    "        elif ((elem in dict_value_stemming)):\n",
    "            netral_count += 1\n",
    "            \n",
    "    feature.append(positive_count)\n",
    "    feature.append(negative_count)\n",
    "    feature.append(netral_count)\n",
    "    \n",
    "    return feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "02dc1e16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 8.88 s, sys: 0 ns, total: 8.88 s\n",
      "Wall time: 8.88 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "x_train_feature_stemmed = [word_array_to_feature_stemming(elem.split(\" \")) for elem in x_train_stemmed]\n",
    "x_test_feature_stemmed = [word_array_to_feature_stemming(elem.split(\" \")) for elem in x_test_stemmed]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7b6e1be0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.04 s, sys: 752 ms, total: 1.79 s\n",
      "Wall time: 258 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "clf_stemmed = LogisticRegression(random_state=random_seed).fit(x_train_feature_stemmed, y_train_stemmed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "873b4eae",
   "metadata": {},
   "source": [
    "Now let's see the results with the modified dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c0eb9369",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_stemmed = clf_stemmed.predict(x_test_feature_stemmed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9a808ba6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For labbel 0 :\n",
      "\t - number of entry :  12500\n",
      "\t - precision :  0.692652329749104\n",
      "\t - recall :  0.6863900548918308\n",
      "\t - fbeta_score :  0.68024\n",
      "\n",
      "\n",
      "For labbel 1 :\n",
      "\t - number of entry :  12500\n",
      "\t - precision :  0.6858692235146181\n",
      "\t - recall :  0.6919600380589915\n",
      "\t - fbeta_score :  0.69816\n"
     ]
    }
   ],
   "source": [
    "print_formated_result(precision_recall_fscore_support(y_test_stemmed, y_pred_stemmed))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e057383",
   "metadata": {},
   "source": [
    "We can see that the results are of the same order of validation but as the claculus time is not significantly longer we will keep this optimization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d72cf3a7",
   "metadata": {},
   "source": [
    "### Lemming"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78558d35",
   "metadata": {},
   "source": [
    "We will now try to use lemming on our text to see if it changes our results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f4dd8da2",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m spacy download en_core_web_sm > output_dl.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "2f7e2f3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading the small English model\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1c0817a",
   "metadata": {},
   "source": [
    "So we can now create the new dict and new feature function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ecd5acb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Open and read all line of the file '''\n",
    "file = open(\"vader_lexicon.txt\", \"r\", encoding=\"utf-8\")\n",
    "lines = file.readlines()\n",
    "file.close()\n",
    "\n",
    "''' We define a list of all our line but in the good format '''\n",
    "good_format_lemming = []\n",
    "\n",
    "''' For each line we replace all tablutaion by space and we split to get the 2 first elements '''\n",
    "for line in lines:\n",
    "    line_temp = line.replace(\"\\t\", \" \")\n",
    "    line_temp = line_temp.split(\" \")\n",
    "    line_temp = line_temp[:2]\n",
    "    line_temp[0] = nlp(line_temp[0])\n",
    "    good_format_lemming.append(line_temp)\n",
    "\n",
    "''' Now we create a dict from our list to use it after '''\n",
    "dict_value_lemming = dict(good_format_lemming)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d6516bfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_array_to_feature_lemming(word_array : list_of_words) -> feature_type:\n",
    "    feature = []\n",
    "    \n",
    "    ''' No feature '''\n",
    "    \n",
    "    if (\"no\" in word_array):\n",
    "        feature.append(1)\n",
    "    else:\n",
    "        feature.append(0)\n",
    "        \n",
    "    ''' Pronouns feature '''\n",
    "    \n",
    "    valid_pronouns = [\"i\", \"me\", \"my\", \"mine\", \"myself\", \"you\", \"your\", \"yours\", \"yourself\", \"we\", \"us\", \"our\", \"ourselves\"]\n",
    "    pronouns_count = 0\n",
    "    for elem in word_array:\n",
    "        if (elem in valid_pronouns):\n",
    "            pronouns_count += 1\n",
    "            \n",
    "    feature.append(pronouns_count)\n",
    "    \n",
    "    ''' \".\" feature '''\n",
    "    \n",
    "    feature.append(word_array.count(\".\"))\n",
    "            \n",
    "    ''' ! feature '''\n",
    "        \n",
    "    if (\"!\" in word_array):\n",
    "        feature.append(1)\n",
    "    else:\n",
    "        feature.append(0)\n",
    "        \n",
    "    ''' log(nb_word) feature '''\n",
    "    \n",
    "    feature.append(math.log(len(word_array)))\n",
    "    \n",
    "    ''' positive, negative and neutral feature '''\n",
    "    positive_count = 0\n",
    "    negative_count = 0\n",
    "    netral_count = 0\n",
    "    \n",
    "    for elem in word_array :\n",
    "        if ((elem in dict_value_lemming) and (float(dict_value_lemming[elem]) >= 1.5)):\n",
    "            positive_count += 1\n",
    "        elif ((elem in dict_value_lemming) and (float(dict_value_lemming[elem]) <= -1.5)):\n",
    "            negative_count += 1\n",
    "        elif ((elem in dict_value_lemming)):\n",
    "            netral_count += 1\n",
    "            \n",
    "    feature.append(positive_count)\n",
    "    feature.append(negative_count)\n",
    "    feature.append(netral_count)\n",
    "    \n",
    "    return feature"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e4143eb",
   "metadata": {},
   "source": [
    "We can now define an other function that take this time the lemming transformation instead of stemming."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "dd419101",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataset_to_array_lemmed(dataset : DataFrame) -> (List[list_of_words], List[int], List[list_of_words], List[int]):\n",
    "    '''\n",
    "        This function take the dataset and create and array from this dataset\n",
    "        \n",
    "        Parameters :\n",
    "                dataset (dataset): it's the input dataset thta we want to format in array\n",
    "                \n",
    "        Returns:\n",
    "                x_train (list[list[str]]) : We have all the list avec all word of text in the train part of the datatset\n",
    "                y_train (list[int]) : We have a list of all label of text in the train part  of the dataset\n",
    "                x_test (list[list[str]]) : We have all the list avec all word of text in the test part of the datatset\n",
    "                y_test (list[int]) : We have a list of all label of text in the test part  of the dataset\n",
    "    '''\n",
    "    x_train = []\n",
    "    y_train = []\n",
    "    x_test = []\n",
    "    y_test = []\n",
    "    \n",
    "    for elem in dataset[\"train\"]:\n",
    "        y_train.append(elem[\"label\"])\n",
    "        x_train.append(' '.join([token.lemma_ for token in nlp(elem[\"text\"])]))\n",
    "        \n",
    "    for elem in dataset[\"test\"]:\n",
    "        y_test.append(elem[\"label\"])\n",
    "        x_test.append(' '.join([token.lemma_ for token in nlp(elem[\"text\"])]))\n",
    "        \n",
    "    return x_train, y_train, x_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b3a8d71a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 22min 37s, sys: 4.84 s, total: 22min 41s\n",
      "Wall time: 22min 42s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "x_train_lemmed, y_train_lemmed, x_test_lemmed, y_test_lemmed = dataset_to_array_lemmed(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "bc0e2691",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4.27 s, sys: 20.6 ms, total: 4.29 s\n",
      "Wall time: 4.28 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "x_train_feature_lemmed = [word_array_to_feature_lemming(elem.split(\" \")) for elem in x_train_lemmed]\n",
    "x_test_feature_lemmed = [word_array_to_feature_lemming(elem.split(\" \")) for elem in x_test_lemmed]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "04c0c418",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 629 ms, sys: 400 ms, total: 1.03 s\n",
      "Wall time: 160 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "clf_lemmed = LogisticRegression(random_state=random_seed).fit(x_train_feature_lemmed, y_train_lemmed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "883d6558",
   "metadata": {},
   "source": [
    "Add now the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "cb0a6656",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_lemmed = clf_lemmed.predict(x_test_feature_lemmed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "d5760806",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For labbel 0 :\n",
      "\t - number of entry :  12500\n",
      "\t - precision :  0.5998505976095617\n",
      "\t - recall :  0.46931618936294567\n",
      "\t - fbeta_score :  0.38544\n",
      "\n",
      "\n",
      "For labbel 1 :\n",
      "\t - number of entry :  12500\n",
      "\t - precision :  0.5472654408297972\n",
      "\t - recall :  0.630242975430976\n",
      "\t - fbeta_score :  0.74288\n"
     ]
    }
   ],
   "source": [
    "print_formated_result(precision_recall_fscore_support(y_test_lemmed, y_pred_lemmed))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c5bb808",
   "metadata": {},
   "source": [
    "So we can see that the results are "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fef9a7a",
   "metadata": {},
   "source": [
    "## Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e64216df",
   "metadata": {},
   "source": [
    "We will use the different regulation on the first model to comprare the performances."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "940d639d",
   "metadata": {},
   "source": [
    "### regularization L1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "75728c99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For labbel 0 :\n",
      "\t - number of entry :  12500\n",
      "\t - precision :  0.6965906303654648\n",
      "\t - recall :  0.6890137883627835\n",
      "\t - fbeta_score :  0.6816\n",
      "\n",
      "\n",
      "For labbel 1 :\n",
      "\t - number of entry :  12500\n",
      "\t - precision :  0.6883076200172292\n",
      "\t - recall :  0.6956349677470418\n",
      "\t - fbeta_score :  0.70312\n",
      "CPU times: user 1.34 s, sys: 1.09 s, total: 2.44 s\n",
      "Wall time: 318 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "clf_l1 = LogisticRegression(random_state=random_seed).fit(x_train_feature, y_train)\n",
    "y_pred_l1 = clf_l1.predict(x_test_feature)\n",
    "print_formated_result(precision_recall_fscore_support(y_test, y_pred_l1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61adf32c",
   "metadata": {},
   "source": [
    "### regularization L2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "0a0f2bde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For labbel 0 :\n",
      "\t - number of entry :  12500\n",
      "\t - precision :  0.6965906303654648\n",
      "\t - recall :  0.6890137883627835\n",
      "\t - fbeta_score :  0.6816\n",
      "\n",
      "\n",
      "For labbel 1 :\n",
      "\t - number of entry :  12500\n",
      "\t - precision :  0.6883076200172292\n",
      "\t - recall :  0.6956349677470418\n",
      "\t - fbeta_score :  0.70312\n",
      "CPU times: user 1.42 s, sys: 1.17 s, total: 2.59 s\n",
      "Wall time: 324 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "clf_l2 = LogisticRegression(random_state=random_seed).fit(x_train_feature, y_train)\n",
    "y_pred_l2 = clf_l2.predict(x_test_feature)\n",
    "print_formated_result(precision_recall_fscore_support(y_test, y_pred_l2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45fd5900",
   "metadata": {},
   "source": [
    "### Regularization Elasticnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "c88807d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For labbel 0 :\n",
      "\t - number of entry :  12500\n",
      "\t - precision :  0.6965906303654648\n",
      "\t - recall :  0.6890137883627835\n",
      "\t - fbeta_score :  0.6816\n",
      "\n",
      "\n",
      "For labbel 1 :\n",
      "\t - number of entry :  12500\n",
      "\t - precision :  0.6883076200172292\n",
      "\t - recall :  0.6956349677470418\n",
      "\t - fbeta_score :  0.70312\n",
      "CPU times: user 1.33 s, sys: 1.15 s, total: 2.49 s\n",
      "Wall time: 311 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "clf_elasticnet = LogisticRegression(random_state=random_seed).fit(x_train_feature, y_train)\n",
    "y_pred_elasticnet = clf_elasticnet.predict(x_test_feature)\n",
    "print_formated_result(precision_recall_fscore_support(y_test, y_pred_elasticnet))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3b09358",
   "metadata": {},
   "source": [
    "### No regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "75b7ed04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For labbel 0 :\n",
      "\t - number of entry :  12500\n",
      "\t - precision :  0.6965906303654648\n",
      "\t - recall :  0.6890137883627835\n",
      "\t - fbeta_score :  0.6816\n",
      "\n",
      "\n",
      "For labbel 1 :\n",
      "\t - number of entry :  12500\n",
      "\t - precision :  0.6883076200172292\n",
      "\t - recall :  0.6956349677470418\n",
      "\t - fbeta_score :  0.70312\n"
     ]
    }
   ],
   "source": [
    "clf_without_regularization = LogisticRegression(random_state=random_seed).fit(x_train_feature, y_train)\n",
    "y_pred_without_regularization = clf_without_regularization.predict(x_test_feature)\n",
    "print_formated_result(precision_recall_fscore_support(y_test, y_pred_without_regularization))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce18316a",
   "metadata": {},
   "source": [
    "So we can see that the regularization doesn't change the result, so we'll stay on the proposed default regularization function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "337596a1",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15dedbf3",
   "metadata": {},
   "source": [
    "Our better results is only with the clean text optimiszation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "bd1b0900",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For labbel 0 :\n",
      "\t - number of entry :  12500\n",
      "\t - precision :  0.6965906303654648\n",
      "\t - recall :  0.6890137883627835\n",
      "\t - fbeta_score :  0.6816\n",
      "\n",
      "\n",
      "For labbel 1 :\n",
      "\t - number of entry :  12500\n",
      "\t - precision :  0.6883076200172292\n",
      "\t - recall :  0.6956349677470418\n",
      "\t - fbeta_score :  0.70312\n"
     ]
    }
   ],
   "source": [
    "print_formated_result(precision_recall_fscore_support(y_test_clean, y_pred_clean))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "886962f7",
   "metadata": {},
   "source": [
    "We can now check why some text are wrongly classified :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "980e5490",
   "metadata": {},
   "outputs": [],
   "source": [
    "wrong_0_index = []\n",
    "wrong_1_index = []\n",
    "\n",
    "for i in range(len(y_pred_clean)):\n",
    "    if ((y_pred_clean[i] == 0) and (y_test_clean[i] == 1)):\n",
    "        wrong_0_index.append(i)\n",
    "    elif ((y_pred_clean[i] == 1) and (y_test_clean[i] == 0)):\n",
    "        wrong_1_index.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "f4f62d98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3711"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(wrong_0_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "bc0ec2bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3980"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(wrong_1_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6f0f13c",
   "metadata": {},
   "source": [
    "2 examples of wrong 0 label :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "9818b3a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'GUTS OF A BEAUTY is a bit better than its predecessor GUTS OF A VIRGIN. Although this film isn\\'t really a sequel in the sense that it has absolutely nothing to do with the first installment, I did find BEAUTY to be a little stronger and better put together all-the-way-around than VIRGIN...but then again, that\\'s not really saying much.<br /><br />BEAUTY starts off as a pretty rough and straight-faced exploit film. A couple of Yakuza cats are holding a young woman prisoner and begin gang raping her in pretty brutal fashion. As this nastiness is going on, the head guy tells the girl that they did the same to her sister and sold her into slavery in Africa, and that they\\'re gonna do the same to her. They then shoot her up with some drugs and rape her some more. She somehow gets away and ends up at a clinic where the nurse there listens to her sob story. The rapee ends up freaking out from the stress of her prior experience and commits suicide. The clinic worker, moved by the young lady\\'s story, decides to take revenge on the gang by seducing one of the lower-level guys and trying to hypnotize him to make him kill the Yakuza leaders. This whole plan backfires, so now Ms. Vigilante-Clinic-Worker gets exposed to much the same treatment that our original rapee got - only worse (some pretty rough butt-rape ensues along with the pre-requisite gang rape...). She too is drugged, but the drug has a strange side effect on our seemingly hapless victim ----- it turns her into a raging hermaphroditic BLOOD DEMON!!! (no sh!t, that\\'s what really happens!!!) This is when BEAUTY really takes off with some pretty f!cking insane kill scenes - including a very classy chest-burst-rape that looks like a cross between ALIEN and a bad porn, and my favorite - a head-engulfed-by-demon-vagina kill (complete with demon vagina-slime...)that has to be seen to be believed...<br /><br />Definitely some promising stuff going on in GUTS OF A BEAUTY, but still very disjointed feeling. BEAUTY almost feels like two different films being forced together in a non-compatible way. Still, I have to give the film credit - the rape scenes are very rough and misogynistic, and the kill scenes are just totally off the wall. A solid 7/10 for another crazy J-horror \"classic\".'"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[\"test\"][int(random.choice(wrong_0_index))][\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "246da19d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"With documentary films, the question of realism always crops up. How much of the film is real and how much is manipulated by the film maker? In LITTLE DIETER NEEDS TO FLY, Herzog is far too absorbed in telling the story of a man telling his own story to even address the question of realism versus formalism. From the beginning, Herzog's role as storyteller is obvious. Luckily, he is a master storyteller. LITTLE DIETER is the finest, most engaging documentary I have ever seen. Dieter's story is enthralling, and Herzog's efforts at reenactment, putting Dieter through the paces of reliving his story on location while it is being filmed, are very effective. The story that Dieter tells is real, but Herzog is ever-present, wrenching absurdist commentary from the realism. This film is a must-see for any students of documentary film and/or of Werner Herzog.\""
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[\"test\"][int(random.choice(wrong_0_index))][\"text\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf70dc2e",
   "metadata": {},
   "source": [
    "2 examples of wrong 1 label :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "17cb8f3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"This Asterix is very similar to modern Disney cartoons. Soulless, technically good and the usual in-jokes for adults. Maybe it's because this is the first cartoon I watched after Laputa: Castle in the Sky, but it was quite disappointing.<br /><br />The plot is contrived and forgettable but it involves Asterix and Obelix going to the Viking's territory to rescue a spoilt teenager who then learns humility and finds love as well. Oh and initially they don't get on but after facing adversity they all share a deep bond of friendship... yadda yadda.<br /><br />The best bit is to watch out for the little jokes. The Vikings get all the best ones. Such as Vikea (the Viking's chief's wife) giving a list of furniture and skulls to bring back from the next raid. Or the Vikings not knowing the meaning of mercy (literally). Oh, and Olaf the dumbest Viking is actually hilarious (as much for the voice acting as the dialogue).<br /><br />For example, aboard the Viking ship: (After a speech by Abba, the captain's daughter) Olaf: Who is this new guy? Captain: That's my daughter, cod-brain! Olaf: Your... daughter's... a man?\""
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[\"test\"][int(random.choice(wrong_1_index))][\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "19429405",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'(As a note, I\\'d like to say that I saw this movie at my annual church camp, where the entire youth group laughed at it. I bought it when I saw it on a shelf one year later, if only for the humor I derived from a bad attempt at making an evangelical movie.)<br /><br />Lay it Down falls short of many movie fans\\' expectations on several different planes. Most of the problems lie within the impersonal acting. Regardless of the nice cars in the film, or the truth in Christ\\'s sacrifice for you, as a movie AND witnessing tool, Lay it Down hardly delivers. <br /><br />Most good opinions of the movies are supported by Christians agreeing with the message. While it\\'s easy for a Christian to agree with the points delivered, the audience hardly ever witnesses life outside a clichÃ©. The fighting scene between Ben and his brother is horribly dubbed. And there are at least three blatant typos in the subtitles.<br /><br />I encourage anyone to watch the movie a second time with the director\\'s commentary on. It really helps you understand just why the movie was written how it was. The director\\'s views on secular society are practically opposite of what would cater to a movie-goer\\'s needs: he shows a pedantic understanding of Nonchristians, as well as some points of religious conflict; most of the editing, he admits, was rushed, but \"satisfactory\"; he thought the over-used transitions and themes to be effective; and was completely happy with the acting. <br /><br />He also inserted motifs that he was rather proud of: -All (read: most) of the names are significant. Ben Destin = \"Been Destined\", Gus Pelman = \"Gospel Man\", Nicky D = Nicodemus. -The car doing donuts is symbolic of the circling nothingness that is a life without Christ. -When Ben leaves on Pete\\'s motorcycle, he crosses his crutches to form a \"cross\".<br /><br />I\\'m not making any of those up. He throws around things like this in between saying while street racers and the like \"blow their brains out with guns\", and how \"God is in control when your born and when your die\". Yes, that was not a typo. He really says that.<br /><br />I have (little) forgiveness reserved for this movie. The \"cool cars\" and \"good message\" don\\'t do jack to make this movie good. However, the movie was made from a group of unprofessional individuals on a budget less than 1/100th of \"The Fast and the Furious\\'s\", and the time limit was unforgiving. With that in mind, I give it a score of 2/10, instead of the 1/10 I so dearly think it deserves.'"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[\"test\"][int(random.choice(wrong_1_index))][\"text\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12ee411e",
   "metadata": {},
   "source": [
    "With these examples, and others that we have studied, we can easily see some problems that our classifier may have:\n",
    "\n",
    "- It does not detect irony in sentences, i.e. if a person uses irony there is a high chance that his text will be classified in the other category.\n",
    "- It also doesn't detect false compliments, for example if someone says \"This is a great ninja movie for kids\" it will just keep the positive side when it can mean that it is an interesting movie when you have a childish vision of the story.\n",
    "- When critics use certain expressions, it can also be a misclassification, often expressions with several meanings that are not necessarily well understood.\n",
    "- When the text is too descriptive, it takes over the possible short passages with the emphasis on the feeling and so these cases are at the border of the 2 classes and so are sometimes misclassified.\n",
    "\n",
    "These are the major problems, there are surely other less visible ones that we have not seen yet. But overall we have a result close to 70% which for a method that only takes a few minutes to execute is quite correct from our point of view."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
