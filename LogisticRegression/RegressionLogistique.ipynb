{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "46cda421",
   "metadata": {},
   "source": [
    "# Logistique Regression model  (TP nÂ°1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22c4981c",
   "metadata": {},
   "source": [
    "## Import"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "058e2761",
   "metadata": {},
   "source": [
    "First of all we need to import several packages.\n",
    "\n",
    "- dataset : we need it to download the database\n",
    "- math : we need it for several math function\n",
    "- sklearn : we need 'LogisticRegression' to compute it on the data and we also need 'precision_recall_fscore_support'for compute  and see results of our model. \n",
    "- numpy : for set the random seed to make result reproducible\n",
    "- pandas : for dataframe manipulation\n",
    "- typing : for type the function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5104d738",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "import math\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import List"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d804783b",
   "metadata": {},
   "source": [
    "## The dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d98e609b",
   "metadata": {},
   "source": [
    "We download the dataset :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "25c2cc67",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset imdb (/home/leherlemaxime/.cache/huggingface/datasets/imdb/plain_text/1.0.0/e3c66f1788a67a89c7058d97ff62b6c30531e05b549de56d3ab91891f0561f9a)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51a66f874bc94a1780b34899f1e5d074",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = datasets.load_dataset('imdb')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e10f4b5",
   "metadata": {},
   "source": [
    "Now take a lokk at the dataset format :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2e541258",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 25000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 25000\n",
       "    })\n",
       "    unsupervised: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 50000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "052a1b9d",
   "metadata": {},
   "source": [
    "So we can see that the dataset is compose of 2 part \"train\" and \"test\". Each part have the same numbers of elements 25000. All element have 2 component, the first id the text and the second is the label : 0 for positive element and 0 for negative element So we will use the \"train\" part to train our model and the \"test\" part to check the performance of our model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc158850",
   "metadata": {},
   "source": [
    "We can see how look at a element of our dataset :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "02885564",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': 'Titanic has to be one of my all-time favorite movies. It has its problems (what movies don\\'t) but still, it\\'s enjoyable.<br /><br />When I stumble across someone who asks me why I like Titanic, I suppose my first reaction is \"wait a minute, you don\\'t?\" I know so many people who don\\'t like this movie, and I\\'m not saying I don\\'t see why. \"The love story is too cheesy\" well, yes but isn\\'t it enjoyable and moving? All right, the love story between Jack and Rose is very unrealistic, everyone knows that love like this doesn\\'t actually exist. But this is a movie, doesn\\'t everyone enjoy watching a beautiful story that lets us slip slightly into fantasy for a while? The next complaint, DiCaprio and Winslet are terrible actors. Well, OK, in this movie, I agree that they do not perform to their full potentials. However I think it\\'s unfair to say that they are terrible actors. I personally think they are both very talented actors who unfortunately are very famous for a movie that they are not amazing in. But the roles they are given are simple, and the characters seem real enough that you can care about them quite a bit, but I agree with many people that they did not do as well as could have been expected.<br /><br />And finally, if one is going to complain that they don\\'t like this movie because they hate romance, or because they hate history, or tragic movies, then I\\'m sorry but why on earth did they go and see a movie that is so clearly all of these things. It\\'s like people who complain The Dark Knight is a bad movie because they hate action movies. Simply for being a movie, not because you dislike the genre, this IS a good movie.<br /><br />Well deserving of its Oscars, in particular, Best Cinematography, which I find to be the best I\\'ve ever seen in a movie save maybe the Lord of the Rings trilogy.<br /><br />I know some of the writing fails, such as the constant screaming of each other\\'s names throughout the movie. The flashback portion of the story can be quite weak at times, but overall it\\'s an amazing achievement in making the Titanic look so real, and the sinking feel so epic.<br /><br />I understand why a lot of people dislike this movie, but for the most part it boils down to them disliking the fundamental idea, such as it being a love story, rather than them thinking the movie in and of itself is poorly constructed.<br /><br />I can tell you that I have read more than five books about the Titanic, including memoirs form the day it happened, and this movie is extremely historically accurate save just a few faults. The only main ones I can find is that the piping should be threaded copper, not steel, and the iceberg looks fairly unrealistic as is the scene where they hit it.<br /><br />I give this movie 10/10, not because I like romance movies, but simply because it\\'s an outstanding cinematic achievement, that leaves one feeling horrified by the realistic adaptation of events.',\n",
       " 'label': 1}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[\"train\"][42]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eed904b",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa931e5e",
   "metadata": {},
   "source": [
    "set a defined random generator, better for reproducible results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "66da1a2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_seed = 42\n",
    "random = np.random.default_rng(random_seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62caf5c6",
   "metadata": {},
   "source": [
    "Later in our model we will have to compute several feature on each text and for this we need a dict were word are associate with their meaning (positive or negative). And in the file 'vader_lexicon.txt' we have for a lot's of word their positiv score so we to convert the file to a dict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bb26885e",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Open and read all line of the file '''\n",
    "file = open(\"vader_lexicon.txt\", \"r\", encoding=\"utf-8\")\n",
    "lines = file.readlines()\n",
    "file.close()\n",
    "\n",
    "''' We define a list of all our line but in the good format '''\n",
    "good_format = []\n",
    "\n",
    "''' For each line we replace all tablutaion by space and we split to get the 2 first elements '''\n",
    "for line in lines:\n",
    "    line_temp = line.replace(\"\\t\", \" \")\n",
    "    line_temp = line_temp.split(\" \")\n",
    "    line_temp = line_temp[:2]\n",
    "    good_format.append(line_temp)\n",
    "\n",
    "''' Now we create a dict from our list to use it after '''\n",
    "dict_value = dict(good_format)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "451f411e",
   "metadata": {},
   "source": [
    "As we can see before, the text-format is not perfect, we have for exemple '\\t' or '<br\\>' that are formated text. So here we just need to have a list of all word (or char) to use them to compute our features. So for this we will replace all special char by space. And we will also add space before and after '!' to use the split function of python for string split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5e18e161",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_the_text(text_array : str) -> List[str]:\n",
    "    '''\n",
    "        This function return a list of all word and char in the text in parameters.\n",
    "\n",
    "            Parameters:\n",
    "                    text_array (str): The text in a string format.\n",
    "\n",
    "            Returns:\n",
    "                    result_array (list[str]) : A list with all the word and char in the inpt text.\n",
    "    '''\n",
    "    \n",
    "    specialChars = \"()\\\\\\''.,;:\\\"?-\" \n",
    "    for specialChar in specialChars:\n",
    "        text_array = text_array.replace(specialChar, ' ')\n",
    "        \n",
    "    text_array = text_array.replace(\"/>\", ' ')\n",
    "    text_array = text_array.replace(\"<br\", ' ')\n",
    "    ''' As say before we add space before and after '!' for the split function '''\n",
    "    text_array = text_array.replace(\"!\", \" ! \")\n",
    "    \n",
    "    ''' We split the text by the space to get a list of all the word in the text'''\n",
    "    text_array = text_array.split(\" \")\n",
    "    \n",
    "    ''' We put all word in lowercase to copare them to the word in the dict'''\n",
    "    result_array = [elem.lower() for elem in text_array if(len(elem) != 0)]\n",
    "    \n",
    "    return result_array"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d84ef37",
   "metadata": {},
   "source": [
    "We need a function that can take our dataset and make it usable for our other function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "781f34f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "DataFrame = pd.core.frame.DataFrame\n",
    "list_of_words = List[str]\n",
    "\n",
    "def dataset_to_array(dataset : DataFrame) -> (List[list_of_words], List[int], List[list_of_words], List[int]):\n",
    "    '''\n",
    "        This function take the dataset and create and array from this dataset\n",
    "        \n",
    "        Parameters :\n",
    "                dataset (dataset): it's the input dataset thta we want to format in array\n",
    "                \n",
    "        Returns:\n",
    "                x_train (list[list[str]]) : We have all the list avec all word of text in the train part of the datatset\n",
    "                y_train (list[int]) : We have a list of all label of text in the train part  of the dataset\n",
    "                x_test (list[list[str]]) : We have all the list avec all word of text in the test part of the datatset\n",
    "                y_test (list[int]) : We have a list of all label of text in the test part  of the dataset\n",
    "    '''\n",
    "    x_train = []\n",
    "    y_train = []\n",
    "    x_test = []\n",
    "    y_test = []\n",
    "    \n",
    "    for elem in dataset[\"train\"]:\n",
    "        y_train.append(elem[\"label\"])\n",
    "        x_train.append(clean_the_text(elem[\"text\"]))\n",
    "        \n",
    "    for elem in dataset[\"test\"]:\n",
    "        y_test.append(elem[\"label\"])\n",
    "        x_test.append(clean_the_text(elem[\"text\"]))\n",
    "        \n",
    "    return x_train, y_train, x_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "969ac921",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train, x_test, y_test = dataset_to_array(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "046e9795",
   "metadata": {},
   "source": [
    "## Creation of the features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "664fe182",
   "metadata": {},
   "source": [
    "As we say before we now need to create feature from our textn we have the followinf feature :\n",
    "\n",
    "- 1 if \"no\" appear in the doc, 0 otherwise\n",
    "- The count of first and second pronouns in the document\n",
    "- 1 if \"!\" is in the document, 0 otherwise\n",
    "- log(word count in the document)\n",
    "- Number of words in the document which are in the positive lexicon\n",
    "- Number of words in the document which are in the negative lexicon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ba379f19",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_type = List[int]\n",
    "\n",
    "def word_array_to_feature(word_array : list_of_words) -> feature_type:\n",
    "    feature = []\n",
    "    \n",
    "    ''' No feature '''\n",
    "    \n",
    "    if (\"no\" in word_array):\n",
    "        feature.append(1)\n",
    "    else:\n",
    "        feature.append(0)\n",
    "        \n",
    "    ''' Pronouns feature '''\n",
    "    \n",
    "    valid_pronouns = [\"i\", \"me\", \"my\", \"mine\", \"myself\", \"you\", \"your\", \"yours\", \"yourself\", \"we\", \"us\", \"our\", \"ourselves\"]\n",
    "    pronouns_count = 0\n",
    "    for elem in word_array:\n",
    "        if (elem in valid_pronouns):\n",
    "            pronouns_count += 1\n",
    "            \n",
    "    feature.append(pronouns_count)\n",
    "            \n",
    "    ''' ! feature '''\n",
    "        \n",
    "    if (\"!\" in word_array):\n",
    "        feature.append(1)\n",
    "    else:\n",
    "        feature.append(0)\n",
    "        \n",
    "    ''' log(nb_word) feature '''\n",
    "    \n",
    "    feature.append(math.log(len(word_array)))\n",
    "    \n",
    "    ''' positive and negative feature '''\n",
    "    positive_count = 0\n",
    "    negative_count = 0\n",
    "    \n",
    "    for elem in word_array :\n",
    "        if ((elem in dict_value) and (float(dict_value[elem]) >= 1.5)):\n",
    "            positive_count += 1\n",
    "        elif ((elem in dict_value) and (float(dict_value[elem]) <= -1.5)):\n",
    "            negative_count += 1\n",
    "            \n",
    "    feature.append(positive_count)\n",
    "    feature.append(negative_count)\n",
    "    \n",
    "    return feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6748ed26",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_feature = [word_array_to_feature(elem) for elem in x_train]\n",
    "x_test_feature = [word_array_to_feature(elem) for elem in x_test]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "595ecfb8",
   "metadata": {},
   "source": [
    "## Creation and train of the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1969599e",
   "metadata": {},
   "source": [
    "We create a 'LogisticRegression' model from sikitleran and we train it with the train data.\n",
    "We also set the random_state to the variable random_seed to control the random and make the result reproducible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "50a322ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = LogisticRegression(random_state=random_seed).fit(x_train_feature, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ee544f9",
   "metadata": {},
   "source": [
    "## Test our model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93980346",
   "metadata": {},
   "source": [
    "We will predict the label of all text in the test part of dataset and we compare with the real result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cb35cde2",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = clf.predict(x_test_feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a8f517a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0.70999516, 0.70619946]),\n",
       " array([0.70352, 0.71264]),\n",
       " array([0.70674275, 0.70940511]),\n",
       " array([12500, 12500]))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precision_recall_fscore_support(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "672d4824",
   "metadata": {},
   "source": [
    "First of all we can see that we have 12500 elements of each labels.\n",
    "\n",
    "So we can see that the result are quite correct. We have a precission of __0,71__ on each label. A recall of __0,70__ for label 0 and __0,71__ for label 1. The fbeta_score, which is the weighted harmonic mean of precision and recall, is of __0,71__ for each label."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
