{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "46cda421",
   "metadata": {},
   "source": [
    "# Logistique Regression model  (TP nÂ°1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22c4981c",
   "metadata": {},
   "source": [
    "## Import"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "058e2761",
   "metadata": {},
   "source": [
    "First of all we need to import several packages.\n",
    "\n",
    "- dataset : we need it to download the database\n",
    "- math : we need it for several math function\n",
    "- sklearn : we need 'LogisticRegression' to compute it on the data and we also need 'precision_recall_fscore_support'for compute  and see results of our model. \n",
    "- numpy : for set the random seed to make result reproducible\n",
    "- pandas : for dataframe manipulation\n",
    "- typing : for type the function\n",
    "- nltk and spacy : for lemming and stemming\n",
    "- re : for use the regex expression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "5104d738",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "import math\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import List\n",
    "from typing import Callable\n",
    "from nltk import tokenize\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "import spacy\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d804783b",
   "metadata": {},
   "source": [
    "## The dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d98e609b",
   "metadata": {},
   "source": [
    "We download the dataset :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "25c2cc67",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset imdb (/home/leherlemaxime/.cache/huggingface/datasets/imdb/plain_text/1.0.0/e3c66f1788a67a89c7058d97ff62b6c30531e05b549de56d3ab91891f0561f9a)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "18b5de8f77044791b3f5102f82748587",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = datasets.load_dataset('imdb')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e10f4b5",
   "metadata": {},
   "source": [
    "Now take a lokk at the dataset format :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2e541258",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 25000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 25000\n",
       "    })\n",
       "    unsupervised: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 50000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "052a1b9d",
   "metadata": {},
   "source": [
    "So we can see that the dataset is compose of 2 part \"train\" and \"test\". Each part have the same numbers of elements 25000. All element have 2 component, the first id the text and the second is the label : 0 for positive element and 0 for negative element So we will use the \"train\" part to train our model and the \"test\" part to check the performance of our model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc158850",
   "metadata": {},
   "source": [
    "We can see how look at a element of our dataset :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "02885564",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': 'Titanic has to be one of my all-time favorite movies. It has its problems (what movies don\\'t) but still, it\\'s enjoyable.<br /><br />When I stumble across someone who asks me why I like Titanic, I suppose my first reaction is \"wait a minute, you don\\'t?\" I know so many people who don\\'t like this movie, and I\\'m not saying I don\\'t see why. \"The love story is too cheesy\" well, yes but isn\\'t it enjoyable and moving? All right, the love story between Jack and Rose is very unrealistic, everyone knows that love like this doesn\\'t actually exist. But this is a movie, doesn\\'t everyone enjoy watching a beautiful story that lets us slip slightly into fantasy for a while? The next complaint, DiCaprio and Winslet are terrible actors. Well, OK, in this movie, I agree that they do not perform to their full potentials. However I think it\\'s unfair to say that they are terrible actors. I personally think they are both very talented actors who unfortunately are very famous for a movie that they are not amazing in. But the roles they are given are simple, and the characters seem real enough that you can care about them quite a bit, but I agree with many people that they did not do as well as could have been expected.<br /><br />And finally, if one is going to complain that they don\\'t like this movie because they hate romance, or because they hate history, or tragic movies, then I\\'m sorry but why on earth did they go and see a movie that is so clearly all of these things. It\\'s like people who complain The Dark Knight is a bad movie because they hate action movies. Simply for being a movie, not because you dislike the genre, this IS a good movie.<br /><br />Well deserving of its Oscars, in particular, Best Cinematography, which I find to be the best I\\'ve ever seen in a movie save maybe the Lord of the Rings trilogy.<br /><br />I know some of the writing fails, such as the constant screaming of each other\\'s names throughout the movie. The flashback portion of the story can be quite weak at times, but overall it\\'s an amazing achievement in making the Titanic look so real, and the sinking feel so epic.<br /><br />I understand why a lot of people dislike this movie, but for the most part it boils down to them disliking the fundamental idea, such as it being a love story, rather than them thinking the movie in and of itself is poorly constructed.<br /><br />I can tell you that I have read more than five books about the Titanic, including memoirs form the day it happened, and this movie is extremely historically accurate save just a few faults. The only main ones I can find is that the piping should be threaded copper, not steel, and the iceberg looks fairly unrealistic as is the scene where they hit it.<br /><br />I give this movie 10/10, not because I like romance movies, but simply because it\\'s an outstanding cinematic achievement, that leaves one feeling horrified by the realistic adaptation of events.',\n",
       " 'label': 1}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[\"train\"][42]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b9dad34",
   "metadata": {},
   "source": [
    "And we also need to define the type that we will use in the following notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "86e15c4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_type = List[int]\n",
    "DataFrame = pd.core.frame.DataFrame\n",
    "list_of_words = List[str]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64436d69",
   "metadata": {},
   "source": [
    "## Random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d3c44e6",
   "metadata": {},
   "source": [
    "set a defined random generator, better for reproducible results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "349b36b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_seed = 38\n",
    "random = np.random.default_rng(random_seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3026cf1",
   "metadata": {},
   "source": [
    "## First model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "851e7307",
   "metadata": {},
   "source": [
    "### Creation of the features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7128ddba",
   "metadata": {},
   "source": [
    "As we say before we now need to create feature from our textn we have the followinf feature :\n",
    "\n",
    "- 1 if \"no\" appear in the doc, 0 otherwise\n",
    "- The count of first and second pronouns in the document\n",
    "- The count of \".\" => number of classic sentences\n",
    "- 1 if \"!\" is in the document, 0 otherwise\n",
    "- log(word count in the document)\n",
    "- Number of words in the document which are in the positive lexicon\n",
    "- Number of words in the document which are in the negative lexicon\n",
    "- Number of words in the document wich are in the lexcion but netral"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2e837020",
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_array_to_feature(word_array : list_of_words) -> feature_type:\n",
    "    feature = []\n",
    "    \n",
    "    ''' No feature '''\n",
    "    \n",
    "    if (\"no\" in word_array):\n",
    "        feature.append(1)\n",
    "    else:\n",
    "        feature.append(0)\n",
    "        \n",
    "    ''' Pronouns feature '''\n",
    "    \n",
    "    valid_pronouns = [\"i\", \"me\", \"my\", \"mine\", \"myself\", \"you\", \"your\", \"yours\", \"yourself\", \"we\", \"us\", \"our\", \"ourselves\"]\n",
    "    pronouns_count = 0\n",
    "    for elem in word_array:\n",
    "        if (elem in valid_pronouns):\n",
    "            pronouns_count += 1\n",
    "            \n",
    "    feature.append(pronouns_count)\n",
    "    \n",
    "    ''' \".\" feature '''\n",
    "    \n",
    "    feature.append(word_array.count(\".\"))\n",
    "            \n",
    "    ''' ! feature '''\n",
    "        \n",
    "    if (\"!\" in word_array):\n",
    "        feature.append(1)\n",
    "    else:\n",
    "        feature.append(0)\n",
    "        \n",
    "    ''' log(nb_word) feature '''\n",
    "    \n",
    "    feature.append(math.log(len(word_array)))\n",
    "    \n",
    "    ''' positive, negative and neutral feature '''\n",
    "    positive_count = 0\n",
    "    negative_count = 0\n",
    "    netral_count = 0\n",
    "    \n",
    "    for elem in word_array :\n",
    "        if ((elem in dict_value) and (float(dict_value[elem]) >= 1.5)):\n",
    "            positive_count += 1\n",
    "        elif ((elem in dict_value) and (float(dict_value[elem]) <= -1.5)):\n",
    "            negative_count += 1\n",
    "        elif ((elem in dict_value)):\n",
    "            netral_count += 1\n",
    "            \n",
    "    feature.append(positive_count)\n",
    "    feature.append(negative_count)\n",
    "    feature.append(netral_count)\n",
    "    \n",
    "    return feature"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74e6b4f4",
   "metadata": {},
   "source": [
    "### Create the lexicon"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98d946e3",
   "metadata": {},
   "source": [
    "We have speak about a lexcion so now we need to create it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "23c3e6ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Open and read all line of the file '''\n",
    "file = open(\"vader_lexicon.txt\", \"r\", encoding=\"utf-8\")\n",
    "lines = file.readlines()\n",
    "file.close()\n",
    "\n",
    "''' We define a list of all our line but in the good format '''\n",
    "good_format = []\n",
    "\n",
    "''' For each line we replace all tablutaion by space and we split to get the 2 first elements '''\n",
    "for line in lines:\n",
    "    line_temp = line.replace(\"\\t\", \" \")\n",
    "    line_temp = line_temp.split(\" \")\n",
    "    line_temp = line_temp[:2]\n",
    "    good_format.append(line_temp)\n",
    "\n",
    "''' Now we create a dict from our list to use it after '''\n",
    "dict_value = dict(good_format)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81a928a6",
   "metadata": {},
   "source": [
    "### Formatting the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "616e9354",
   "metadata": {},
   "source": [
    "We need a function that can take our dataset and make it usable for our other function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "044f3bab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataset_to_array(dataset : DataFrame) -> (List[list_of_words], List[int], List[list_of_words], List[int]):\n",
    "    '''\n",
    "        This function take the dataset and create and array from this dataset\n",
    "        \n",
    "        Parameters :\n",
    "                dataset (dataset): it's the input dataset thta we want to format in array\n",
    "                \n",
    "        Returns:\n",
    "                x_train (list[list[str]]) : We have all the list avec all word of text in the train part of the datatset\n",
    "                y_train (list[int]) : We have a list of all label of text in the train part  of the dataset\n",
    "                x_test (list[list[str]]) : We have all the list avec all word of text in the test part of the datatset\n",
    "                y_test (list[int]) : We have a list of all label of text in the test part  of the dataset\n",
    "    '''\n",
    "    x_train = []\n",
    "    y_train = []\n",
    "    x_test = []\n",
    "    y_test = []\n",
    "    \n",
    "    for elem in dataset[\"train\"]:\n",
    "        y_train.append(elem[\"label\"])\n",
    "        x_train.append(elem[\"text\"])\n",
    "        \n",
    "    for elem in dataset[\"test\"]:\n",
    "        y_test.append(elem[\"label\"])\n",
    "        x_test.append(elem[\"text\"])\n",
    "        \n",
    "    return x_train, y_train, x_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7b14963c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.58 s, sys: 58.4 ms, total: 2.63 s\n",
      "Wall time: 2.61 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "x_train, y_train, x_test, y_test = dataset_to_array(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65bd6e9a",
   "metadata": {},
   "source": [
    "We can see that formatting the dataset don't take a lot's of time : 2.22 seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6af87c26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4.24 s, sys: 10.2 ms, total: 4.25 s\n",
      "Wall time: 4.25 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "x_train_feature = [word_array_to_feature(elem.split(\" \")) for elem in x_train]\n",
    "x_test_feature = [word_array_to_feature(elem.split(\" \")) for elem in x_test]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d553db1",
   "metadata": {},
   "source": [
    "The creation of the feature just take 3.34 seconds wich is fast. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1969599e",
   "metadata": {},
   "source": [
    "We create a 'LogisticRegression' model from sikitleran and we train it with the train data.\n",
    "We also set the random_state to the variable random_seed to control the random and make the result reproducible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "50a322ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 704 ms, sys: 575 ms, total: 1.28 s\n",
      "Wall time: 187 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "clf = LogisticRegression(random_state=random_seed).fit(x_train_feature, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ee544f9",
   "metadata": {},
   "source": [
    "#### Result of the first model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cb35cde2",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = clf.predict(x_test_feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4c48b903",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_formated_result(result):\n",
    "    print(\"For labbel 0 :\")\n",
    "    print(\"\\t - number of entry : \", result[3][0])\n",
    "    print(\"\\t - precision : \", result[0][0])\n",
    "    print(\"\\t - recall : \", result[2][0])\n",
    "    print(\"\\t - fbeta_score : \", result[1][0])\n",
    "    print(\"\\n\\nFor labbel 1 :\")\n",
    "    print(\"\\t - number of entry : \", result[3][1])\n",
    "    print(\"\\t - precision : \", result[0][1])\n",
    "    print(\"\\t - recall : \", result[2][1])\n",
    "    print(\"\\t - fbeta_score : \", result[1][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "752f9751",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For labbel 0 :\n",
      "\t - number of entry :  12500\n",
      "\t - precision :  0.6965906303654648\n",
      "\t - recall :  0.6890137883627835\n",
      "\t - fbeta_score :  0.6816\n",
      "\n",
      "\n",
      "For labbel 1 :\n",
      "\t - number of entry :  12500\n",
      "\t - precision :  0.6883076200172292\n",
      "\t - recall :  0.6956349677470418\n",
      "\t - fbeta_score :  0.70312\n"
     ]
    }
   ],
   "source": [
    "print_formated_result(precision_recall_fscore_support(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "672d4824",
   "metadata": {},
   "source": [
    "First of all we can see that we have 12500 elements of each labels.\n",
    "\n",
    "So we can see that the result are quite correct. We have a precission of __0,69__ on each label. A recall of __0,68__ for label 0 and __0,70__ for label 1. The fbeta_score, which is the weighted harmonic mean of precision and recall, is of __0,79__ for each label."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7eec3be",
   "metadata": {},
   "source": [
    "## Pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7796342b",
   "metadata": {},
   "source": [
    "### Clean the text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f59f23a",
   "metadata": {},
   "source": [
    "As we can see before, the text-format is not perfect, we have for exemple '\\t' or '<br\\>' that are formated text. So here we just need to have a list of all word (or char) to use them to compute our features. So for this we will replace all special char by space. And we will also add space before and after '!' to use the split function of python for string split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e920fe23",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_the_text(text_array : str) -> List[str]:\n",
    "    '''\n",
    "        This function return a list of all word and char in the text in parameters.\n",
    "\n",
    "            Parameters:\n",
    "                    text_array (str): The text in a string format.\n",
    "\n",
    "            Returns:\n",
    "                    result_array (list[str]) : A list with all the word and char in the inpt text.\n",
    "    '''\n",
    "    \n",
    "    specialChars = \"()\\\\\\'',;:\\\"?-\" \n",
    "    for specialChar in specialChars:\n",
    "        text_array = text_array.replace(specialChar, ' ')\n",
    "        \n",
    "    text_array = text_array.replace(\".\", \" . \")\n",
    "    text_array = text_array.replace(\"/>\", ' ')\n",
    "    text_array = text_array.replace(\"<br\", ' ')\n",
    "    ''' As say before we add space before and after '!' for the split function '''\n",
    "    text_array = text_array.replace(\"!\", \" ! \")\n",
    "    \n",
    "    ''' We split the text by the space to get a list of all the word in the text'''\n",
    "    text_array = text_array.split(\" \")\n",
    "    \n",
    "    ''' We put all word in lowercase to copare them to the word in the dict'''\n",
    "    result_array = [elem.lower() for elem in text_array if(len(elem) != 0)]\n",
    "    \n",
    "    return result_array"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f197795b",
   "metadata": {},
   "source": [
    "Now we can create the new function with the clean feature"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a05a87b",
   "metadata": {},
   "source": [
    "This time the formatting take more time : 5.45 seconds when we add 2.22 seconds without optimisation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f2664dcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataset_to_array_clean(dataset : DataFrame) -> (List[list_of_words], List[int], List[list_of_words], List[int]):\n",
    "    '''\n",
    "        This function take the dataset and create and array from this dataset\n",
    "        \n",
    "        Parameters :\n",
    "                dataset (dataset): it's the input dataset thta we want to format in array\n",
    "                \n",
    "        Returns:\n",
    "                x_train (list[list[str]]) : We have all the list avec all word of text in the train part of the datatset\n",
    "                y_train (list[int]) : We have a list of all label of text in the train part  of the dataset\n",
    "                x_test (list[list[str]]) : We have all the list avec all word of text in the test part of the datatset\n",
    "                y_test (list[int]) : We have a list of all label of text in the test part  of the dataset\n",
    "    '''\n",
    "    x_train = []\n",
    "    y_train = []\n",
    "    x_test = []\n",
    "    y_test = []\n",
    "    \n",
    "    for elem in dataset[\"train\"]:\n",
    "        y_train.append(elem[\"label\"])\n",
    "        x_train.append(clean_the_text(elem[\"text\"]))\n",
    "        \n",
    "    for elem in dataset[\"test\"]:\n",
    "        y_test.append(elem[\"label\"])\n",
    "        x_test.append(clean_the_text(elem[\"text\"]))\n",
    "        \n",
    "    return x_train, y_train, x_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3361a040",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.19 s, sys: 101 ms, total: 2.29 s\n",
      "Wall time: 2.26 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "x_train_clean, y_train_clean, x_test_clean, y_test_clean = dataset_to_array(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1420143c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 14 s, sys: 1.1 ms, total: 14 s\n",
      "Wall time: 14 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "x_train_feature_clean = [word_array_to_feature(elem) for elem in x_train_clean]\n",
    "x_test_feature_clean = [word_array_to_feature(elem) for elem in x_test_clean]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5bf5886c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.03 s, sys: 1.47 s, total: 3.5 s\n",
      "Wall time: 460 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "clf_clean = LogisticRegression(random_state=random_seed).fit(x_train_feature_clean, y_train_clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab2f1eee",
   "metadata": {},
   "source": [
    "Now take a look at this result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f48de2c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_clean = clf_clean.predict(x_test_feature_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "95ab8ef3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For labbel 0 :\n",
      "\t - number of entry :  12500\n",
      "\t - precision :  0.5424659346474401\n",
      "\t - recall :  0.5938880440292563\n",
      "\t - fbeta_score :  0.65608\n",
      "\n",
      "\n",
      "For labbel 1 :\n",
      "\t - number of entry :  12500\n",
      "\t - precision :  0.5649666059502125\n",
      "\t - recall :  0.49888303100705933\n",
      "\t - fbeta_score :  0.44664\n"
     ]
    }
   ],
   "source": [
    "print_formated_result(precision_recall_fscore_support(y_test_clean, y_pred_clean))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09070682",
   "metadata": {},
   "source": [
    "First of all we can see that we have 12500 elements of each labels. This don't change from the previous model.\n",
    "\n",
    "So we can see that the result are quite correct. We have a precission of __0,71__ on each label. A recall of __0,70__ for label 0 and __0,71__ for label 1. The fbeta_score, which is the weighted harmonic mean of precision and recall, is of __0,71__ for each label."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20fa54ca",
   "metadata": {},
   "source": [
    "### Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4b33c106",
   "metadata": {},
   "outputs": [],
   "source": [
    "re_word = re.compile(r\"^\\w+$\")\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "\n",
    "stem_word: Callable[[str], str] = lambda w : stemmer.stem(w.lower()) if re_word.match(w) else w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "839012c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stemmed_text(text : str) -> str:\n",
    "    '''\n",
    "        This function steeming the text in parameter and return in\n",
    "        \n",
    "        Parameters :\n",
    "                text (str) : the text to stemming\n",
    "                \n",
    "        Returns :\n",
    "                return_text (str) : the text stemmed\n",
    "    '''\n",
    "    list_of_words = text.split(\" \")\n",
    "    \n",
    "    list_of_words = [stem_word(word) for word in list_of_words]\n",
    "    \n",
    "    return_text = \" \".join(list_of_words)\n",
    "    \n",
    "    return return_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31e018cd",
   "metadata": {},
   "source": [
    "Now define an other function for this optimisation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7ce37752",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataset_to_array_stemmed(dataset : DataFrame) -> (List[list_of_words], List[int], List[list_of_words], List[int]):\n",
    "    '''\n",
    "        This function take the dataset and create and array from this dataset\n",
    "        \n",
    "        Parameters :\n",
    "                dataset (dataset): it's the input dataset thta we want to format in array\n",
    "                \n",
    "        Returns:\n",
    "                x_train (list[list[str]]) : We have all the list avec all word of text in the train part of the datatset\n",
    "                y_train (list[int]) : We have a list of all label of text in the train part  of the dataset\n",
    "                x_test (list[list[str]]) : We have all the list avec all word of text in the test part of the datatset\n",
    "                y_test (list[int]) : We have a list of all label of text in the test part  of the dataset\n",
    "    '''\n",
    "    x_train = []\n",
    "    y_train = []\n",
    "    x_test = []\n",
    "    y_test = []\n",
    "    \n",
    "    for elem in dataset[\"train\"]:\n",
    "        y_train.append(elem[\"label\"])\n",
    "        x_train.append(stemmed_text(elem[\"text\"]))\n",
    "        \n",
    "    for elem in dataset[\"test\"]:\n",
    "        y_test.append(elem[\"label\"])\n",
    "        x_test.append(stemmed_text(elem[\"text\"]))\n",
    "        \n",
    "    return x_train, y_train, x_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "af4bd6c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 7s, sys: 305 ms, total: 1min 8s\n",
      "Wall time: 1min 7s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "x_train_stemmed, y_train_stemmed, x_test_stemmed, y_test_stemmed = dataset_to_array_stemmed(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "05a0ad85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.98 s, sys: 18.8 ms, total: 4 s\n",
      "Wall time: 4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "x_train_feature_stemmed = [word_array_to_feature(elem.split(\" \")) for elem in x_train_stemmed]\n",
    "x_test_feature_stemmed = [word_array_to_feature(elem.split(\" \")) for elem in x_test_stemmed]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "bde63c3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 342 ms, sys: 442 ms, total: 784 ms\n",
      "Wall time: 123 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "clf_stemmed = LogisticRegression(random_state=random_seed).fit(x_train_feature_stemmed, y_train_stemmed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9de1fc01",
   "metadata": {},
   "source": [
    "We can now test out new optimisation and see if it's change the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3a0d8428",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_stemmed = clf_stemmed.predict(x_test_feature_stemmed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "557920ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For labbel 0 :\n",
      "\t - number of entry :  12500\n",
      "\t - precision :  0.6702224427354668\n",
      "\t - recall :  0.6591306469320538\n",
      "\t - fbeta_score :  0.6484\n",
      "\n",
      "\n",
      "For labbel 1 :\n",
      "\t - number of entry :  12500\n",
      "\t - precision :  0.6594871000232432\n",
      "\t - recall :  0.6700515605935372\n",
      "\t - fbeta_score :  0.68096\n"
     ]
    }
   ],
   "source": [
    "print_formated_result(precision_recall_fscore_support(y_test_stemmed, y_pred_stemmed))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5f247c1",
   "metadata": {},
   "source": [
    "So we can see that the results are "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d72cf3a7",
   "metadata": {},
   "source": [
    "### Lemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "f4dd8da2",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m spacy download en_core_web_sm > output_dl.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "2f7e2f3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading the small English model\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e4143eb",
   "metadata": {},
   "source": [
    "We can now define an other function that take this time the lemming transformation instead of stemming."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "dd419101",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataset_to_array_lemmed(dataset : DataFrame) -> (List[list_of_words], List[int], List[list_of_words], List[int]):\n",
    "    '''\n",
    "        This function take the dataset and create and array from this dataset\n",
    "        \n",
    "        Parameters :\n",
    "                dataset (dataset): it's the input dataset thta we want to format in array\n",
    "                \n",
    "        Returns:\n",
    "                x_train (list[list[str]]) : We have all the list avec all word of text in the train part of the datatset\n",
    "                y_train (list[int]) : We have a list of all label of text in the train part  of the dataset\n",
    "                x_test (list[list[str]]) : We have all the list avec all word of text in the test part of the datatset\n",
    "                y_test (list[int]) : We have a list of all label of text in the test part  of the dataset\n",
    "    '''\n",
    "    x_train = []\n",
    "    y_train = []\n",
    "    x_test = []\n",
    "    y_test = []\n",
    "    \n",
    "    for elem in dataset[\"train\"]:\n",
    "        y_train.append(elem[\"label\"])\n",
    "        x_train.append(' '.join([token.lemma_ for token in nlp(elem[\"text\"])]))\n",
    "        \n",
    "    for elem in dataset[\"test\"]:\n",
    "        y_test.append(elem[\"label\"])\n",
    "        x_test.append(' '.join([token.lemma_ for token in nlp(elem[\"text\"])]))\n",
    "        \n",
    "    return x_train, y_train, x_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "b3a8d71a",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_333/3979605755.py\u001b[0m in \u001b[0;36mdataset_to_array_lemmed\u001b[0;34m(dataset)\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0melem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"train\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0my_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melem\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"label\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m         \u001b[0mx_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlemma_\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnlp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melem\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"text\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0melem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"test\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/NLPSentimentDectection/venv/lib/python3.8/site-packages/spacy/language.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, text, disable, component_cfg)\u001b[0m\n\u001b[1;32m    997\u001b[0m                 \u001b[0merror_handler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mproc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_error_handler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    998\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 999\u001b[0;31m                 \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mproc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcomponent_cfg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1000\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1001\u001b[0m                 \u001b[0;31m# This typically happens if a component is not initialized\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/NLPSentimentDectection/venv/lib/python3.8/site-packages/spacy/pipeline/trainable_pipe.pyx\u001b[0m in \u001b[0;36mspacy.pipeline.trainable_pipe.TrainablePipe.__call__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/NLPSentimentDectection/venv/lib/python3.8/site-packages/spacy/pipeline/transition_parser.pyx\u001b[0m in \u001b[0;36mspacy.pipeline.transition_parser.Parser.predict\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/NLPSentimentDectection/venv/lib/python3.8/site-packages/spacy/pipeline/transition_parser.pyx\u001b[0m in \u001b[0;36mspacy.pipeline.transition_parser.Parser.greedy_parse\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/NLPSentimentDectection/venv/lib/python3.8/site-packages/thinc/model.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    313\u001b[0m         \u001b[0monly\u001b[0m \u001b[0mthe\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minstead\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    314\u001b[0m         \"\"\"\n\u001b[0;32m--> 315\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_train\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    316\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    317\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfinish_update\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptimizer\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/NLPSentimentDectection/venv/lib/python3.8/site-packages/spacy/ml/tb_framework.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(model, X, is_train)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m     step_model = ParserStepModel(\n\u001b[0m\u001b[1;32m     34\u001b[0m         \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/NLPSentimentDectection/venv/lib/python3.8/site-packages/spacy/ml/parser_model.pyx\u001b[0m in \u001b[0;36mspacy.ml.parser_model.ParserStepModel.__init__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/NLPSentimentDectection/venv/lib/python3.8/site-packages/thinc/model.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, X, is_train)\u001b[0m\n\u001b[1;32m    289\u001b[0m         \"\"\"Call the model's `forward` function, returning the output and a\n\u001b[1;32m    290\u001b[0m         callback to compute the gradients via backpropagation.\"\"\"\n\u001b[0;32m--> 291\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_train\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mis_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    292\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    293\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0minitialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mInT\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mOutT\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;34m\"Model\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/NLPSentimentDectection/venv/lib/python3.8/site-packages/thinc/layers/chain.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(model, X, is_train)\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0mcallbacks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m         \u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minc_layer_grad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_train\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mis_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m         \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minc_layer_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/NLPSentimentDectection/venv/lib/python3.8/site-packages/thinc/model.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, X, is_train)\u001b[0m\n\u001b[1;32m    289\u001b[0m         \"\"\"Call the model's `forward` function, returning the output and a\n\u001b[1;32m    290\u001b[0m         callback to compute the gradients via backpropagation.\"\"\"\n\u001b[0;32m--> 291\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_train\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mis_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    292\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    293\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0minitialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mInT\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mOutT\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;34m\"Model\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/NLPSentimentDectection/venv/lib/python3.8/site-packages/thinc/layers/chain.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(model, X, is_train)\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0mcallbacks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m         \u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minc_layer_grad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_train\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mis_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m         \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minc_layer_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/NLPSentimentDectection/venv/lib/python3.8/site-packages/thinc/model.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, X, is_train)\u001b[0m\n\u001b[1;32m    289\u001b[0m         \"\"\"Call the model's `forward` function, returning the output and a\n\u001b[1;32m    290\u001b[0m         callback to compute the gradients via backpropagation.\"\"\"\n\u001b[0;32m--> 291\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_train\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mis_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    292\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    293\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0minitialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mInT\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mOutT\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;34m\"Model\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/NLPSentimentDectection/venv/lib/python3.8/site-packages/thinc/layers/with_array.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(model, Xseq, is_train)\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXseq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_list_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mModel\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mList2d\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mList2d\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mXseq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/NLPSentimentDectection/venv/lib/python3.8/site-packages/thinc/layers/with_array.py\u001b[0m in \u001b[0;36m_list_forward\u001b[0;34m(model, Xs, is_train)\u001b[0m\n\u001b[1;32m     74\u001b[0m     \u001b[0mlengths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray1i\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseq\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mseq\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mXs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m     \u001b[0mXf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpad\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpad\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m     \u001b[0mYf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mget_dXf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mbackprop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdYs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mList2d\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mList2d\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/NLPSentimentDectection/venv/lib/python3.8/site-packages/thinc/model.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, X, is_train)\u001b[0m\n\u001b[1;32m    289\u001b[0m         \"\"\"Call the model's `forward` function, returning the output and a\n\u001b[1;32m    290\u001b[0m         callback to compute the gradients via backpropagation.\"\"\"\n\u001b[0;32m--> 291\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_train\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mis_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    292\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    293\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0minitialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mInT\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mOutT\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;34m\"Model\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/NLPSentimentDectection/venv/lib/python3.8/site-packages/thinc/layers/chain.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(model, X, is_train)\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0mcallbacks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m         \u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minc_layer_grad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_train\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mis_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m         \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minc_layer_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/NLPSentimentDectection/venv/lib/python3.8/site-packages/thinc/model.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, X, is_train)\u001b[0m\n\u001b[1;32m    289\u001b[0m         \"\"\"Call the model's `forward` function, returning the output and a\n\u001b[1;32m    290\u001b[0m         callback to compute the gradients via backpropagation.\"\"\"\n\u001b[0;32m--> 291\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_train\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mis_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    292\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    293\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0minitialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mInT\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mOutT\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;34m\"Model\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/NLPSentimentDectection/venv/lib/python3.8/site-packages/thinc/layers/residual.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(model, X, is_train)\u001b[0m\n\u001b[1;32m     38\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0md_output\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mdX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m     \u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbackprop_layer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbackprop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/NLPSentimentDectection/venv/lib/python3.8/site-packages/thinc/model.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, X, is_train)\u001b[0m\n\u001b[1;32m    289\u001b[0m         \"\"\"Call the model's `forward` function, returning the output and a\n\u001b[1;32m    290\u001b[0m         callback to compute the gradients via backpropagation.\"\"\"\n\u001b[0;32m--> 291\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_train\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mis_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    292\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    293\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0minitialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mInT\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mOutT\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;34m\"Model\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/NLPSentimentDectection/venv/lib/python3.8/site-packages/thinc/layers/chain.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(model, X, is_train)\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0mcallbacks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m         \u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minc_layer_grad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_train\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mis_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m         \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minc_layer_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/NLPSentimentDectection/venv/lib/python3.8/site-packages/thinc/model.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, X, is_train)\u001b[0m\n\u001b[1;32m    289\u001b[0m         \"\"\"Call the model's `forward` function, returning the output and a\n\u001b[1;32m    290\u001b[0m         callback to compute the gradients via backpropagation.\"\"\"\n\u001b[0;32m--> 291\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_train\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mis_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    292\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    293\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0minitialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mInT\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mOutT\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;34m\"Model\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/NLPSentimentDectection/venv/lib/python3.8/site-packages/thinc/layers/chain.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(model, X, is_train)\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0mcallbacks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m         \u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minc_layer_grad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_train\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mis_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m         \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minc_layer_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/NLPSentimentDectection/venv/lib/python3.8/site-packages/thinc/model.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, X, is_train)\u001b[0m\n\u001b[1;32m    289\u001b[0m         \"\"\"Call the model's `forward` function, returning the output and a\n\u001b[1;32m    290\u001b[0m         callback to compute the gradients via backpropagation.\"\"\"\n\u001b[0;32m--> 291\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_train\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mis_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    292\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    293\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0minitialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mInT\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mOutT\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;34m\"Model\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/NLPSentimentDectection/venv/lib/python3.8/site-packages/thinc/layers/chain.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(model, X, is_train)\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0mcallbacks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m         \u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minc_layer_grad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_train\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mis_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m         \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minc_layer_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/NLPSentimentDectection/venv/lib/python3.8/site-packages/thinc/model.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, X, is_train)\u001b[0m\n\u001b[1;32m    289\u001b[0m         \"\"\"Call the model's `forward` function, returning the output and a\n\u001b[1;32m    290\u001b[0m         callback to compute the gradients via backpropagation.\"\"\"\n\u001b[0;32m--> 291\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_train\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mis_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    292\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    293\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0minitialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mInT\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mOutT\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;34m\"Model\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/NLPSentimentDectection/venv/lib/python3.8/site-packages/thinc/layers/maxout.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(model, X, is_train)\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0mW\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_param\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"W\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[0mW\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape2f\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mW\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnO\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnP\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnI\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m     \u001b[0mY\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgemm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mW\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrans2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m     \u001b[0mY\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape1f\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnO\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnP\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0mZ\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape3f\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnO\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnP\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "x_train_lemmed, y_train_lemmed, x_test_lemmed, y_test_lemmed = dataset_to_array_lemmed(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "bc0e2691",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'x_train_lemmed' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'x_train_lemmed' is not defined"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "x_train_feature_lemmed = [word_array_to_feature(elem.split(\" \")) for elem in x_train_lemmed]\n",
    "x_test_feature_lemmed = [word_array_to_feature(elem.split(\" \")) for elem in x_test_lemmed]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "04c0c418",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'x_train_feature_lemmed' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'x_train_feature_lemmed' is not defined"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "clf_lemmed = LogisticRegression(random_state=random_seed).fit(x_train_feature_lemmed, y_train_lemmed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "883d6558",
   "metadata": {},
   "source": [
    "Add now the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "cb0a6656",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'clf_lemmed' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_333/2459224230.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0my_pred_lemmed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclf_lemmed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test_feature_lemmed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'clf_lemmed' is not defined"
     ]
    }
   ],
   "source": [
    "y_pred_lemmed = clf_lemmed.predict(x_test_feature_lemmed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5760806",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_formated_result(precision_recall_fscore_support(y_test_lemmed, y_pred_lemmed))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c5bb808",
   "metadata": {},
   "source": [
    "So we can see that the results are "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fef9a7a",
   "metadata": {},
   "source": [
    "## Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e64216df",
   "metadata": {},
   "source": [
    "We win use the different regulation on the first model to comprare the performances."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "940d639d",
   "metadata": {},
   "source": [
    "### regularization L1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "75728c99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For labbel 0 :\n",
      "\t - number of entry :  12500\n",
      "\t - precision :  0.6965906303654648\n",
      "\t - recall :  0.6890137883627835\n",
      "\t - fbeta_score :  0.6816\n",
      "\n",
      "\n",
      "For labbel 1 :\n",
      "\t - number of entry :  12500\n",
      "\t - precision :  0.6883076200172292\n",
      "\t - recall :  0.6956349677470418\n",
      "\t - fbeta_score :  0.70312\n",
      "CPU times: user 1.16 s, sys: 833 ms, total: 2 s\n",
      "Wall time: 283 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "clf_l1 = LogisticRegression(random_state=random_seed).fit(x_train_feature, y_train)\n",
    "y_pred_l1 = clf_l1.predict(x_test_feature)\n",
    "print_formated_result(precision_recall_fscore_support(y_test, y_pred_l1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61adf32c",
   "metadata": {},
   "source": [
    "### regularization L2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "0a0f2bde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For labbel 0 :\n",
      "\t - number of entry :  12500\n",
      "\t - precision :  0.6965906303654648\n",
      "\t - recall :  0.6890137883627835\n",
      "\t - fbeta_score :  0.6816\n",
      "\n",
      "\n",
      "For labbel 1 :\n",
      "\t - number of entry :  12500\n",
      "\t - precision :  0.6883076200172292\n",
      "\t - recall :  0.6956349677470418\n",
      "\t - fbeta_score :  0.70312\n",
      "CPU times: user 1.06 s, sys: 1.01 s, total: 2.07 s\n",
      "Wall time: 284 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "clf_l2 = LogisticRegression(random_state=random_seed).fit(x_train_feature, y_train)\n",
    "y_pred_l2 = clf_l2.predict(x_test_feature)\n",
    "print_formated_result(precision_recall_fscore_support(y_test, y_pred_l2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45fd5900",
   "metadata": {},
   "source": [
    "### Regularization Elasticnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "c88807d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For labbel 0 :\n",
      "\t - number of entry :  12500\n",
      "\t - precision :  0.6965906303654648\n",
      "\t - recall :  0.6890137883627835\n",
      "\t - fbeta_score :  0.6816\n",
      "\n",
      "\n",
      "For labbel 1 :\n",
      "\t - number of entry :  12500\n",
      "\t - precision :  0.6883076200172292\n",
      "\t - recall :  0.6956349677470418\n",
      "\t - fbeta_score :  0.70312\n",
      "CPU times: user 1.19 s, sys: 922 ms, total: 2.11 s\n",
      "Wall time: 288 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "clf_elasticnet = LogisticRegression(random_state=random_seed).fit(x_train_feature, y_train)\n",
    "y_pred_elasticnet = clf_elasticnet.predict(x_test_feature)\n",
    "print_formated_result(precision_recall_fscore_support(y_test, y_pred_elasticnet))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3b09358",
   "metadata": {},
   "source": [
    "### No regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "75b7ed04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For labbel 0 :\n",
      "\t - number of entry :  12500\n",
      "\t - precision :  0.6965906303654648\n",
      "\t - recall :  0.6890137883627835\n",
      "\t - fbeta_score :  0.6816\n",
      "\n",
      "\n",
      "For labbel 1 :\n",
      "\t - number of entry :  12500\n",
      "\t - precision :  0.6883076200172292\n",
      "\t - recall :  0.6956349677470418\n",
      "\t - fbeta_score :  0.70312\n"
     ]
    }
   ],
   "source": [
    "clf_without_regularization = LogisticRegression(random_state=random_seed).fit(x_train_feature, y_train)\n",
    "y_pred_without_regularization = clf_without_regularization.predict(x_test_feature)\n",
    "print_formated_result(precision_recall_fscore_support(y_test, y_pred_without_regularization))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce18316a",
   "metadata": {},
   "source": [
    "So we can see that the best regularization is"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "337596a1",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15dedbf3",
   "metadata": {},
   "source": [
    "Our better results is for lemming."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd1b0900",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb336985",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "980e5490",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4f62d98",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
